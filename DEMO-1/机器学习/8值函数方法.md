---
date: 2025-11-01
---
用**函数**表示状态值/动作值
值函数法人工神经网络和深度学习结合起来
#### 表格&函数
 假设有$n$个状态$\{s_i\}_{i=1}^n$。对于一个给定的策略$\pi$，其状态值为$\{v_\pi(s_i)\}_{i=1}^n$。设$\{\hat{v}(s_i)\}_{i=1}^n$为状态值的估计值。 
 **表格法**：表格可以以数组或者向量的形式存储在内存中。如果要检索或更新一个状态值，我们可以直接读取或重写表格中的相应元素。

| 状态     | $s_1$          | $s_2$          | $\cdots$ | $s_n$          |
| ------ | -------------- | -------------- | -------- | -------------- |
| 估计的状态值 | $\hat{v}(s_1)$ | $\hat{v}(s_2)$ | $\cdots$ | $\hat{v}(s_n)$ |

**函数法**：注意到$\{(s_i, \hat{v}(s_i))\}_{i=1}^n$是一组点，这些点可以通过一条曲线来拟合或近似。最简单的曲线是一条直线，可以描述为 $$ \hat{v}(s, w) = as + b = \underbrace{[s, 1]}_{\phi^{\mathrm{T}}(s)} \underbrace{\begin{bmatrix} a \\ b \end{bmatrix}}_{w} = \phi^{\mathrm{T}}(s)w. \tag{8.1} $$ 其中$\hat{v}(s, w)$是用来近似$v_\pi(s)$的函数，它由状态$s$和参数向量$w \in \mathbb{R}^2$共同决定。有时被写成$\hat{v}_w(s)$。$\phi(s) \in \mathbb{R}^2$被称为$s$的**特征向量**（feature vector）。 
**线性函数近似**
##### 检索和更新值。
**检索一个状态值**：
**表格**：直接读取表格中相应的元素。表格法需要存储$n$个值
**函数**：如果想检索一个状态的值，我们要将状态$s$输入到函数中，然后计算函数的值。 ![图8.3 使用函数检索$s$对应的值的过程](https://via.placeholder.com/200x100?text=函数+输入s+参数w+输出%5Chat%7Ev(s%2Cw)) 得益于上述检索方式，函数法在存储方面更为高效。函数法只需要存储一个低维参数向量$w$。函数法是通过牺牲准确性来提高存储效率的。 
**如何更新一个值**：
**表格**：直接重写表格中对应的元素。
**函数**：必须更新函数的参数$w$从而间接地改变值。函数法在泛化能力方面比表格法更强（当使用表格法时，如果某一个状态被访问过，那么我们可以根据后续轨迹的回报来更新它的值。如果一个状态从来没有被访问过，它的值当然无法更新。当使用函数法时，我们需要通过更新$w$来更新一个状态的值。$w$的改变当然也会影响其他一些状态的值）。
### 基于值函数的时序差分算法；状态值估计
令$v_\pi(s)$和$\hat{v}(s, w)$分别代表状态$s \in \mathcal{S}$的真实状态值和估计状态值。我们的任务是找到一个最优的$w$，从而使得$\hat{v}(s, w)$能够最好地近似每一个$s$的$v_\pi(s)$。目标函数是 $$ J(w) = \mathbb{E}\left[(v_\pi(S) - \hat{v}(S, w))^2\right], \tag{8.3} $$ 其中$S \in \mathcal{S}$是随机变量。
- **均匀分布（uniform distribution）**$$ J(w) = \frac{1}{n}\sum_{s \in \mathcal{S}}(v_\pi(s) - \hat{v}(s, w))^2. \tag{8.4} $$ 
- **平稳分布（stationary distribution）**
  设$\{d_\pi(s)\}_{s \in \mathcal{S}}$为在策略$\pi$下的平稳分布，即经过相当长的时间后，智能体在状态$s$的概率是$d_\pi(s)$，根据定义有$\sum_{s \in \mathcal{S}} d_\pi(s) = 1$。$$ J(w) = \sum_{s \in \mathcal{S}} d_\pi(s) (v_\pi(s) - \hat{v}(s, w))^2. \tag{8.5} $$
##### 平稳分布
分析平稳分布的核心工具是矩阵$P_\pi \in \mathbb{R}^{n \times n}$，即在给定策略$\pi$下的状态转移概率矩阵。$[P_\pi]_{ij}$是智能体在策略$\pi$下从$s_i$用一步转移到$s_j$的概率。 
- 对$P_\pi^k$的解读（$k=1,2,3,\dots$）$$ p_{ij}^{(k)} = \Pr(S_{t_k} = j | S_{t_0} = i) $$ 表示智能体用$k$步从$s_i$转移到$s_j$的概率。其中$t_0$和$t_k$分别代表初始时刻和$k$时刻。那么根据$P_\pi$的定义可得 $$ [P_\pi]_{ij} = p_{ij}^{(1)}, $$ 即$[P_\pi]_{ij}$是智能体用一步从$s_i$转移到$s_j$的概率。 对于$P_\pi^2$，有 $$ [P_\pi^2]_{ij} = [P_\pi P_\pi]_{ij} = \sum_{q=1}^n [P_\pi]_{iq} [P_\pi]_{qj}. $$ 因为$[P_\pi]_{iq} [P_\pi]_{qj}$等于从$s_i$到$s_q$再从$s_q$到$s_j$的联合转移概率，所以$[P_\pi^2]_{ij}$是用两步从$s_i$转移到$s_j$的概率，即 $$ [P_\pi^2]_{ij} = p_{ij}^{(2)}. $$ 类似地，可得 $$ [P_\pi^k]_{ij} = p_{ij}^{(k)}, $$ 即$[P_\pi^k]_{ij}$是使用恰好$k$步从$s_i$转移到$s_j$的概率。 
- 平稳分布的定义：设$d_0 \in \mathbb{R}^n$是一个向量，代表初始时刻状态的概率分布。设$d_k \in \mathbb{R}^n$是从$d_0$开始经过恰好$k$步后得到的概率分布向量。那么 $$ d_k(s_i) = \sum_{j=1}^n d_0(s_j) [P_\pi^k]_{ji}, \quad i = 1, 2, \dots \tag{8.6} $$ 上式的含义是智能体在$k$时刻转移到$s_i$的概率等于从$\{s_j\}_{j=1}^n$使用$k$步转移到$s_i$的概率之和。
- 矩阵-向量形式是 $$ d_k^{\mathrm{T}} = d_0^{\mathrm{T}} P_\pi^k. \tag{8.7} $$ 考虑马尔可夫过程的长期行为。在某些条件下（稍后会讨论），下式成立： $$ \lim_{k \to \infty} P_\pi^k = \mathbf{1}_n d_\pi^{\mathrm{T}}, \tag{8.8} $$ 其中$\mathbf{1}_n = [1, \dots, 1]^{\mathrm{T}} \in \mathbb{R}^n$，因此$\mathbf{1}_n d_\pi^{\mathrm{T}}$是一个所有行都等于$d_\pi^{\mathrm{T}}$的常数矩阵。 将(8.8)代入(8.7)可得 $$ \lim_{k \to \infty} d_k^{\mathrm{T}} = d_0^{\mathrm{T}} \lim_{k \to \infty} P_\pi^k = d_0^{\mathrm{T}} \mathbf{1}_n d_\pi^{\mathrm{T}} = d_\pi^{\mathrm{T}}, \tag{8.9} $$ 其中最后一个等号成立是因为$d_0^{\mathrm{T}} \mathbf{1}_n = 1$。 式(8.9)意味着状态分布$d_k$会最终收敛到一个常值$d_\pi$，该收敛值称为**极限分布（limit distribution）**。极限分布依赖于系统模型和策略$\pi$，但是与初始分布$d_0$无关。
  $d_\pi$的值可以通过以下方法计算。对等式$d_k^{\mathrm{T}} = d_{k-1}^{\mathrm{T}} P_\pi$两边取极限可得 $$ d_\pi^{\mathrm{T}} = d_\pi^{\mathrm{T}} P_\pi. \tag{8.10} $$ 上式表明$d_\pi$是矩阵$P_\pi$的一个左特征向量，其对应的特征值是1。方程(8.10)的解被称为**平稳分布**，它满足$\sum_{s \in S} d_\pi(s) = 1$且$d_\pi(s) > 0$对所有$s \in S$成立。
- 平稳分布的唯一性条件 方程(8.10)的解$d_\pi$通常被称为平稳分布，而(8.9)的$d_\pi$被称为极限分布。(8.9)可以推出来(8.10)，但反之可能不成立。
- 其次，不可约（irreducible）的马尔可夫过程具有唯一稳态分布，常规（regular）的马尔可夫过程具有唯一极限分布。
###### 基础定义
- 如果存在一个有限自然数$k$使得$[P_\pi^k]_{ij} > 0$，则称从状态$s_i$出发**可达**（accessible）状态$s_j$，即智能体从$s_i$出发有概率能在有限次转移后到达$s_j$。 
- 如果两个状态$s_i$和$s_j$相互可达，则这两个状态称为**互通**（communicate）的。 
- 如果所有状态之间都互通，则这个马尔可夫过程被称为**不可约**（irreducible）的。在直观上，智能体从任意一个状态出发总是有概率在有限步内到达任意其他状态。在数学上，对于任意$s_i$和$s_j$，存在$k \geqslant 1$使得$[P_\pi^k]_{ij} > 0$（不同的$i,j$可能对应不同的$k$值）。 
- 如果存在$k \geqslant 1$使得对所有的$i,j$都有$[P_\pi^k]_{ij} > 0$（即不同的$i,j$对应相同的$k$值），则该马尔可夫过程被称为**常规**（regular）的，即任意状态的概率都能在最多$k$步内从其他任何状态到达。一个等价的定义是存在$k \geqslant 1$使得$P_\pi^k > 0$（这里“$>$”是逐元素比较的）。常规马尔可夫过程也是不可约的，但反之则不成立。不过，如果一个马尔可夫过程是不可约的，并且存在$i$使得$[P_\pi]_{ii} > 0$，那么它也是常规的。此外，如果$P_\pi^k > 0$，那么对于任何$k' \geqslant k$，都有$P_\pi^{k'} > 0$，这是由于$P_\pi \geqslant 0$。此时由式(8.9)可知，$d_\pi(s) > 0$（而不是$d_\pi(s) \geqslant 0$）对于每个$s$都成立。探索型策略产生常规马尔可夫过程。
##### 优化算法
梯度下降算法： $$ w_{k+1} = w_k - \alpha_k \nabla_w J(w_k), $$ 其中的梯度是 $$ \begin{align*} \nabla_w J(w_k) &= \nabla_w \mathbb{E}\left[(v_\pi(S) - \hat{v}(S, w_k))^2\right] \\ &= \mathbb{E}\left[\nabla_w (v_\pi(S) - \hat{v}(S, w_k))^2\right] \\ &= 2\mathbb{E}\left[(v_\pi(S) - \hat{v}(S, w_k))(-\nabla_w \hat{v}(S, w_k))\right] \\ &= -2\mathbb{E}\left[(v_\pi(S) - \hat{v}(S, w_k))\nabla_w \hat{v}(S, w_k)\right]. \end{align*} $$代入梯度下降算法可得 $$ w_{k+1} = w_k + 2\alpha_k \mathbb{E}\left[(v_\pi(S) - \hat{v}(S, w_k))\nabla_w \hat{v}(S, w_k)\right], \tag{8.11} $$
- **蒙特卡罗方法**：如果我们有一个从$s_t$开始的回合数据，设$g_t$为从$s_t$开始的折扣回报，那么$g_t$可以用作$v_\pi(s_t)$的近似值。此时，式(8.12)中的算法变为 $$ w_{t+1} = w_t + \alpha_t (g_t - \hat{v}(s_t, w_t)) \nabla_w \hat{v}(s_t, w_t). $$ 这是基于值函数的蒙特卡罗算法。 
- **时序差分方法**：根据时序差分的思想，我们可以用TD误差$r_{t+1} + \gamma \hat{v}(s_{t+1}, w_t) - \hat{v}(s_t, w_t)$来代替真实误差$v_\pi(s_t) - \hat{v}(s_t, w_t)$。$$ w_{t+1} = w_t + \alpha_t \left[r_{t+1} + \gamma \hat{v}(s_{t+1}, w_t) - \hat{v}(s_t, w_t)\right] \nabla_w \hat{v}(s_t, w_t). \tag{8.13} $$ 这就是基于值函数的TD算法。
###### 伪代码
***
初始化: 参数可微的值函数$\hat{v}(s, w)$。初始参数$w_0$。 
目标: 估计一个给定策略$\pi$的状态值。
- 对于由$\pi$生成的每个回合$\{(s_t, r_{t+1}, s_{t+1})\}_t$ 　　
	- 对于每个样本$(s_t, r_{t+1}, s_{t+1})$ 　　　　
	- 对于一般值函数: $w_{t+1} = w_t + \alpha_t\left[r_{t+1} + \gamma\hat{v}(s_{t+1}, w_t) - \hat{v}(s_t, w_t)\right]\nabla_w \hat{v}(s_t, w_t)$ 　　　　
	- 对于线性值函数: $w_{t+1} = w_t + \alpha_t\left[r_{t+1} + \gamma\phi^{\mathrm{T}}(s_{t+1})w_t - \phi^{\mathrm{T}}(s_t)w_t\right]\phi(s_t)$
***
基于表格的TD算法是基于线性值函数的TD算法的特殊情况
#### 收敛性分析 
考虑如下算法： $$ w_{t+1} = w_t + \alpha_t \mathbb{E}\left[\left(r_{t+1} + \gamma \phi^{\mathrm{T}}(s_{t+1})w_t - \phi^{\mathrm{T}}(s_t)w_t\right)\phi(s_t)\right], \tag{8.19} $$ 其中的期望是针对三个随机变量$s_t, s_{t+1}, r_{t+1}$。假设$s_t$服从平稳分布$d_\pi$，定义 $$ \Phi = \begin{bmatrix} \vdots \\ \phi^{\mathrm{T}}(s) \\ \vdots \end{bmatrix} \in \mathbb{R}^{n \times m}, \quad D = \begin{bmatrix} \ddots & & \\ & d_\pi(s) & \\ & & \ddots \end{bmatrix} \in \mathbb{R}^{n \times n}, \tag{8.20} $$ 其中矩阵$\Phi$的每一行对应一个状态的特征向量，对角阵$D$的对角线元素是平稳分布向量中的元素。
**引理8.1**期望可以重写为 $$ \mathbb{E}\left[\left(r_{t+1} + \gamma \phi^{\mathrm{T}}(s_{t+1})w_t - \phi^{\mathrm{T}}(s_t)w_t\right)\phi(s_t)\right] = b - A w_t, $$
其中 $$ \begin{align*} A &\doteq \Phi^{\mathrm{T}} D (I - \gamma P_\pi) \Phi \in \mathbb{R}^{m \times m}, \\ b &\doteq \Phi^{\mathrm{T}} D r_\pi \in \mathbb{R}^m. \tag{8.21} \end{align*} $$ 这里$P_\pi, r_\pi$是贝尔曼方程$v_\pi = r_\pi + \gamma P_\pi v_\pi$中的两个量，而$I$是具有合适维度的单位矩阵。
根据引理8.1，算法可以重写为 $$ w_{t+1} = w_t + \alpha_t (b - A w_t). \tag{8.22} $$
1. $w^* = A^{-1}b.$
	- $A$不仅可逆，还是（非对称）正定的，即对于任意具有合适维度的非零向量$x$都有$x^{\mathrm{T}} A x > 0$。
	- $w^* = A^{-1}b$  实际上是最小化投影贝尔曼误差（projected Bellman error）的最优解。如果选择特殊的特征向量，基于值函数的TD-Linear算法就退化为基于表格的TD算法。把这个特殊的特征向量代入$w^*$，特征向量为$\phi(s) = [0, \dots, 1, \dots, 0]^{\mathrm{T}}$，代入可得 $$ w^* = A^{-1}b = v_\pi. \tag{8.23} $$此时$\Phi = I$。$A = \Phi^{\mathrm{T}} D (I - \gamma P_\pi) \Phi = D(I - \gamma P_\pi)$，$b = \Phi^{\mathrm{T}} D r_\pi = D r_\pi$，$w^* = A^{-1}b = (I - \gamma P_\pi)^{-1} D^{-1} D r_\pi = (I - \gamma P_\pi)^{-1} r_\pi = v_\pi$。
2. $w_t$会随着$t \to \infty$收敛到$w^* = A^{-1}b$。
#### TD-Linear算法优化的是投影贝尔曼误差 
目标函数
1. $$ J_E(w) = \mathbb{E}\left[(v_\pi(S) - \hat{v}(S, w))^2\right]. $$ 矩阵-向量形式： $$ J_E(w) = \|\hat{v}(w) - v_\pi\|_D^2, $$ 其中$v_\pi$是真实状态值向量，而$\hat{v}(w)$是估计的值向量，这两个向量的每一个元素都对应一个状态。这里$\|\cdot\|_D$是加权范数：$\|x\|_D^2 = x^{\mathrm{T}} D x = \|D^{1/2}x\|_2^2$。该目标函数涉及未知的真实状态值，所以直接优化它是无法得到可行的算法的。
2. 贝尔曼误差（Bellman error）。由于$v_\pi$满足贝尔曼方程$v_\pi = r_\pi + \gamma P_\pi v_\pi$，因此估计值$\hat{v}(w)$也应尽可能满足此方程。贝尔曼误差的定义为 $$ J_{BE}(w) = \|\hat{v}(w) - (r_\pi + \gamma P_\pi \hat{v}(w))\|_D^2 \doteq \|\hat{v}(w) - T_\pi(\hat{v}(w))\|_D^2. \tag{8.30} $$
$T_\pi(\cdot)$是贝尔曼算子：对任意$x \in \mathbb{R}^n$有 $$ T_\pi(x) \doteq r_\pi + \gamma P_\pi x. $$该目标函数可能无法被最小化到0，这是因为函数的近似能力有限，不一定能准确刻画所有状态值，从而无法严格满足一个贝尔曼方程。 
3. 投影贝尔曼误差（projected Bellman error）$$ J_{\mathrm{PBE}}(w) = \|\hat{v}(w) - M T_\pi(\hat{v}(w))\|_D^2, $$ 其中$M \in \mathbb{R}^{n \times n}$是一个正交投影矩阵，它在几何上可将任意向量投影到函数能够近似的值空间上。在(8.13)中的TD算法旨在最小化投影贝尔曼误差$J_{\mathrm{PBE}}$，而不是$J_E$或$J_{\mathrm{BE}}$。而且$J_{\mathrm{PBE}}$一定可以被最小化到0。在线性情况下，$\hat{v}(w) = \Phi w$。$\Phi$的列空间（range space）是该线性函数所有可能取值的集合。此时， $$ M = \Phi (\Phi^{\mathrm{T}} D \Phi)^{-1} \Phi^{\mathrm{T}} D \in \mathbb{R}^{n \times n} \tag{8.31} $$ 是一个可以将任意向量投影到$\Phi$的列空间的投影矩阵。由于$\hat{v}(w)$在$\Phi$的列空间中，因此我们总能找到一个$w$使得$J_{\mathrm{PBE}}(w)$最小化至0。可以证明，最小化$J_{\mathrm{PBE}}(w)$的解就是$w^* = A^{-1}b$，即 $$ w^* = A^{-1}b = \arg\min_w J_{\mathrm{PBE}}(w). $$
 在线性情况下，最小化$J_{\mathrm{PBE}}$的最优估计值是$\hat{v}(w^*) = \Phi w^*$，其与真正的状态值$v_\pi$的误差满足如下不等式： $$ \|\Phi w^* - v_\pi\|_D \leqslant \frac{1}{1 - \gamma} \min_w \|\hat{v}(w) - v_\pi\|_D = \frac{1}{1 - \gamma} \min_w \sqrt{J_E(w)}. \tag{8.33} $$表明$\hat{v}(w^*)$与$v_\pi$之间的误差小于$J_E(w)$的最小值，在一定程度上说明了优化$J_{\mathrm{PBE}}$得到的最优估计值与真实状态值是接近的。不过它给出的上界并不紧致，尤其是当$\gamma$接近于1时，因此其价值主要体现在理论上。
#####  最小二乘时序差分算法
  最小二乘TD（least-squares TD, LSTD）的算法旨在最小化投影贝尔曼误差
 $A$和$b$也可以写成 $$ \begin{align*} A &= \mathbb{E}\left[\phi(s_t)(\phi(s_t) - \gamma \phi(s_{t+1}))^{\mathrm{T}}\right], \\ b &= \mathbb{E}\left[r_{t+1}\phi(s_t)\right]. \end{align*} $$上式中的期望是针对随机变量$s_t$、$s_{t+1}$、$r_{t+1}$而言的。
  LSTD的思路：已知最优解的表达式为$w^* = A^{-1}b$，那么可以使用随机样本直接估计$A$和$b$，假设得到的估计值为$\hat{A}$和$\hat{b}$，之后可以直接得到最优参数的估计$w^* \approx \hat{A}^{-1}\hat{b}$。
  令$\hat{A}_t$、$\hat{b}_t$分别为$t$时刻$A$、$b$的估计值，它们可以通过计算样本的平均值得到： $$ \begin{align*} \hat{A}_t &= \sum_{k=0}^{t-1} \phi(s_k)\left(\phi(s_k) - \gamma \phi(s_{k+1})\right)^{\mathrm{T}}, \\ \hat{b}_t &= \sum_{k=0}^{t-1} r_{k+1}\phi(s_k). \tag{8.35} \end{align*} $$ 因此，在$t$时刻最优参数的估计值为 $$ w_t = \hat{A}_t^{-1}\hat{b}_t. $$
### 基于值函数的时序差分；动作值估计
#### 基于值函数的Sarsa
将(8.13)中的$\hat{v}(s, w)$替换为$\hat{q}(s, a, w)$可得 $$ w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \hat{q}(s_{t+1}, a_{t+1}, w_t) - \hat{q}(s_t, a_t, w_t) \right] \nabla_w \hat{q}(s_t, a_t, w_t). \tag{8.36} $$当使用线性函数时，我们有 $$ \hat{q}(s, a, w) = \phi^{\mathrm{T}}(s, a)w, $$ 其中$\phi(s, a)$是一个特征向量，此时$\nabla_w \hat{q}(s, a, w) = \phi(s, a)$。 算法(8.36)只用来估计状态值，即做策略评价。将其与策略改进步骤相结合，从而学习最优策略。
该算法旨在从预设状态出发到达目标状态的最优策略，全局最优需修改
###### 伪代码
***
初始化: 初始参数$w_0$。初始策略$\pi_0$。对所有$t$，设置$\alpha_t = \alpha > 0$。$\epsilon \in (0, 1)$。 
目标: 学习最优策略从而使智能体能从给定状态$s_0$出发到达目标状态。 
- 对于每个回合 　　
	- 在$s_0$，根据$\pi_0(s_0)$，得到$a_0$ 　　
	- 在时刻$t$，如果$s_t$不是目标状态 　　　　
		- 收集经验样本$(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$：在$s_t$，执行$a_t$，通过与环境交互生成$r_{t+1}, s_{t+1}$，再根据$\pi_t(s_{t+1})$生成$a_{t+1}$ 
		- 更新值: 　　　　
			- $w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \hat{q}(s_{t+1}, a_{t+1}, w_t) - \hat{q}(s_t, a_t, w_t) \right] \nabla_w \hat{q}(s_t, a_t, w_t)$ 　　　　
		- 更新策略: 　　　　
			- $\pi_{t+1}(a|s_t) = 1 - \frac{\epsilon(|\mathcal{A}(s_t)| - 1)}{|\mathcal{A}(s_t)|}$，如果$a = \arg\max_{a \in \mathcal{A}(s_t)} \hat{q}(s_t, a, w_{t+1})$ 　　　　
			- $\pi_{t+1}(a|s_t) = \frac{\epsilon}{|\mathcal{A}(s_t)|}$，如果$a \neq \arg\max_{a \in \mathcal{A}(s_t)} \hat{q}(s_t, a, w_{t+1})$ 　　　　
			- $s_t \leftarrow s_{t+1}, a_t \leftarrow a_{t+1}$
***
#### 基于值函数的Q-learning
基于表格的Q-learning也可以推广到基于函数的Q-learning算法： $$ w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \max_{a \in \mathcal{A}(s_{t+1})} \hat{q}(s_{t+1}, a, w_t) - \hat{q}(s_t, a_t, w_t) \right] \nabla_w \hat{q}(s_t, a_t, w_t). \tag{8.37} $$y与Sarsa算法区别仅在于$\hat{q}(s_{t+1}, a_{t+1}, w_t)$被换成了$\max_{a \in \mathcal{A}(s_{t+1})} \hat{q}(s_{t+1}, a, w_t)$。 
##### On-policy 模式
###### 伪代码
***
初始化: 初始参数$w_0$。初始策略$\pi_0$。对于所有$t$，设置$\alpha_t = \alpha > 0$。$\epsilon \in (0,1)$。 
目标: 学习最优策略从而使智能体能从给定状态$s_0$出发到达目标状态。 
- 对每一个回合 　　
	- 在$t$时刻，如果$s_t$不是目标状态 　　　　
		- 收集经验样本$(s_t, a_t, r_{t+1}, s_{t+1})$：在$s_t$，根据$\pi_t(s_t)$产生$a_t$，通过与环境互动生成$r_{t+1}, s_{t+1}$ 　　　　
		- 更新值: 　　　　
			- $w_{t+1} = w_t + \alpha_t \left[ r_{t+1} + \gamma \max_{a \in \mathcal{A}} \hat{q}(s_{t+1}, a, w_t) - \hat{q}(s_t, a_t, w_t) \right] \nabla_w \hat{q}(s_t, a_t, w_t)$ 　　　　
		- 更新策略: 　　　　
			- $\pi_{t+1}(a|s_t) = 1 - \frac{\epsilon(|\mathcal{A}(s_t)| - 1)}{|\mathcal{A}(s_t)|}$，如果$a = \arg\max_{a \in \mathcal{A}(s_t)} \hat{q}(s_t, a, w_{t+1})$ 　　　　
			- $\pi_{t+1}(a|s_t) = \frac{\epsilon}{|\mathcal{A}(s_t)|}$，如果$a \neq \arg\max_{a \in \mathcal{A}(s_t)} \hat{q}(s_t, a, w_{t+1})$
***
### 深度Q-learning
将深度神经网络整合到Q-learning中，以获得一种称为深度Q-learning（deep Q-learning）或深度Q网络（deep Q-network, DQN）的方法。
深度Q-learning旨在最小化如下目标函数： $$ J = \mathbb{E}\left[\left(R + \gamma \max_{a \in \mathcal{A}(S')} \hat{q}(S', a, w) - \hat{q}(S, A, w)\right)^2\right], \tag{8.38} $$ 其中$(S, A, R, S')$是随机变量，分别表示状态、动作、即时奖励、下一个状态。

它对应了贝尔曼最优误差：当$\hat{q}(S, A, w)$等于最优动作值时，$R + \gamma \max_{a \in \mathcal{A}(S')} \hat{q}(S', a, w) - \hat{q}(S, A, w)$在期望意义上应等于0。这可以由下面的贝尔曼最优方程看出： $$ q(s, a) = \mathbb{E}\left[R_{t+1} + \gamma \max_{a \in \mathcal{A}(S_{t+1})} q(S_{t+1}, a) \bigg| S_t = s, A_t = a\right], \quad \text{对所有} s, a. $$上式是贝尔曼最优方程。从该式可以看出，当$\hat{q}(S, A, w)$等于最优动作值时，$R + \gamma \max_{a \in \mathcal{A}(S')} \hat{q}(S', a, w) - \hat{q}(S, A, w)$在期望意义上等于0。
引入两个网络：一个是用于表示$\hat{q}(s, a, w)$的**主网络（main network）**，另一个是用于表示$\hat{q}(s, a, w_T)$的**目标网络（target network）**。此时，目标函数变为 $$ J = \mathbb{E}\left[\left(R + \gamma \max_{a \in \mathcal{A}(S')} \hat{q}(S', a, w_T) - \hat{q}(S, A, w)\right)^2\right], $$ 当$w_T$固定不变时，容易计算出$J$的梯度为 $$ \nabla_w J = -\mathbb{E}\left[\left(R + \gamma \max_{a \in \mathcal{A}(S')} \hat{q}(S', a, w_T) - \hat{q}(S, A, w)\right) \nabla_w \hat{q}(S, A, w)\right], \tag{8.39} $$
##### 使用技巧
1. 使用两个网络：一个主网络和一个目标网络。令$w$和$w_T$分别表示主网络和目标网络的参数，它们的初始值相同。 
	- 每次迭代会从回放缓冲区（replay buffer）抽取一小批次的样本$\{(s, a, r, s')\}$。主网络的输入是$s$和$a$，输出$y = \hat{q}(s, a, w)$是估计的$q$值，输出的目标值是$y_T \doteq r + \gamma \max_{a \in \mathcal{A}(s')} \hat{q}(s', a, w_T)$。主网络更新是为了最小化样本$\{(s, a, y_T)\}$上的TD误差（也称为损失函数）$\sum (y - y_T)^2$。
	- 更新主网络参数并不是显式地使用(8.39)中的梯度。相反，它需要小批量的样本并基于现有的神经网络训练工具来更新参数，这是和不使用神经网络的一个显著区别。
	- 虽然每次迭代中都会更新主网络，但是目标网络并非每次都更新，而是隔一定数量的迭代后更新为与主网络相同的参数。这样就可以满足计算 (8.39) 中的梯度时$w_T$是固定不变的假设。
2. 经验回放（experience replay） 。在收集了一些经验样本后，我们不会按照它们被收集的顺序使用这些样本，而是将它们存储在一个称为回放缓冲区的集合中。每次更新主网络时，从回放缓冲区抽取小批量的经验样本，这个过程被称为经验回放。
	- 为了定义该目标函数，我们必须指定$S$、$A$、$R$、$S'$的概率分布。当$(S, A)$给定时，$R$和$S'$的分布由系统模型确定。因此，我们只需要指定$(S, A)$的分布。为了满足均匀分布的假设，需要打破序列中样本之间的相关性。使用经验回放技术，按照均匀分布从回放缓冲区随机抽取样本，这是经验回放的必要性和为什么服从均匀分布的理论原因。
	- 经验回放的另一个好处是每个经验样本可能会被多次使用，可以提高数据利用率。 
###### 伪代码（Off-policy）
***
初始化：一个主网络和一个目标网络，它们具有相同的初始参数。 
目标：得到一个目标网络，能从给定行为策略$\pi_0$生成的经验样本中学习最优动作值，进而得到最优策略。
- 将$\pi_0$生成的经验样本存储在回放缓冲区$\mathcal{B} = \{(s, a, r, s')\}$ 
	- 对于每次迭代 
		- 从$\mathcal{B}$中均匀抽取一小批量样本 
		- 对于每个样本$(s, a, r, s')$，计算目标值$y_T = r + \gamma \max_{a \in \mathcal{A}(s')} \hat{q}(s', a, w_T)$，其中$w_T$是目标网络的参数 
		- 使用小批量样本更新主网络，以最小化$(y_T - \hat{q}(s, a, w))^2$ 
	- 每$C$次迭代更新$w_T$为$w_T = w$
***
