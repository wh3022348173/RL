---
date: 2025-10-12
---
回报可以用来评价策略的好坏 
贝尔曼方程的核心思想：从一个状态出发获得的回报依赖于从其他状态出发时获得的回报。
$$ \begin{align*} v_1 &= r_1 + \gamma r_2 + \gamma^2 r_3 + \dots, \\ v_2 &= r_2 + \gamma r_3 + \gamma^2 r_4 + \dots, \\ v_3 &= r_3 + \gamma r_4 + \gamma^2 r_1 + \dots, \\ v_4 &= r_4 + \gamma r_1 + \gamma^2 r_2 + \dots. \end{align*} $$
$$

\begin{cases}

v_1 = r_1 + \gamma(r_2 + \gamma r_3 + \dots) = r_1 + \gamma v_2, \\

v_2 = r_2 + \gamma(r_3 + \gamma r_4 + \dots) = r_2 + \gamma v_3, \\

v_3 = r_3 + \gamma(r_4 + \gamma r_1 + \dots) = r_3 + \gamma v_4, \\

v_4 = r_4 + \gamma(r_1 + \gamma r_2 + \dots) = r_4 + \gamma v_1.

\end{cases}

$$
$$

\underbrace{\begin{bmatrix} v_1 \\ v_2 \\ v_3 \\ v_4 \end{bmatrix}}_{v \in \mathbb{R}^4} = \underbrace{\begin{bmatrix} r_1 \\ r_2 \\ r_3 \\ r_4 \end{bmatrix}}_{r \in \mathbb{R}^4} + \begin{bmatrix} \gamma v_2 \\ \gamma v_3 \\ \gamma v_4 \\ \gamma v_1 \end{bmatrix} = \underbrace{\begin{bmatrix} r_1 \\ r_2 \\ r_3 \\ r_4 \end{bmatrix}}_{r \in \mathbb{R}^4} + \gamma \underbrace{\begin{bmatrix} 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ 1 & 0 & 0 & 0 \end{bmatrix}}_{P \in \mathbb{R}^{4 \times 4}} \underbrace{\begin{bmatrix} v_1 \\ v_2 \\ v_3 \\ v_4 \end{bmatrix}}_{v \in \mathbb{R}^4},

$$

上式可以简化为

$$

v = r + \gamma P v.

$$
#### 状态值
在时刻$t = 0, 1, 2, \dots$，智能体处于状态$S_t$，按照策略$\pi$采取动作$A_t$，转移到的下一个状态是$S_{t+1}$，获得的即时奖励是$R_{t+1}$。这个过程可以简洁地表达为

$$S_t \stackrel{A_t}{\rightarrow} S_{t+1}, R_{t+1}.$$
根据定义，沿这个轨迹得到
#### 折扣回报
$$G_t = R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3} + …$$
由于$G_t$是一个随机变量，因此我们可以计算它的期望值 (expectation或expected value) 为
$$v_\pi(s) \triangleq \mathbb{E}[G_t | S_t = s].$$

#### 状态值函数 (state-value function) 
一般简称为状态值或状态价值(state value)

- 第一，$v_\pi(s)$的值依赖于s, 即不同状态的状态值一般是不同的
- 第二，$v_\pi(s)$的值依赖于$\pi$, 即不同策略对应的状态值一般是不同的
- 第三，$v_\pi(s)$并不依赖于t
#### 贝尔曼方程
$$

\begin{align*}

G_t &= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \\

&= R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \\

&= R_{t+1} + \gamma G_{t+1},

\end{align*}

$$
$$

\begin{align*}

v_\pi(s) &= \mathbb{E}[G_t | S_t = s] \\

&= \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s] \\

&= \mathbb{E}[R_{t+1} | S_t = s] + \gamma \mathbb{E}[G_{t+1} | S_t = s].

\end{align*}

$$

$$

\begin{align*}

v_\pi(s) &= \mathbb{E}[R_{t+1} | S_t = s] + \gamma \mathbb{E}[G_{t+1} | S_t = s] \\

&= \underbrace{\sum_{a \in \mathcal{A}} \pi(a|s) \sum_{r \in \mathcal{R}} p(r|s,a) r}_{\text{即时奖励的期望}} + \underbrace{\gamma \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{s' \in \mathcal{S}} p(s'|s,a) v_\pi(s')}_{\text{未来奖励的期望}} \\

&= \sum_{a \in \mathcal{A}} \pi(a|s) \left[ \sum_{r \in \mathcal{R}} p(r|s,a) r + \gamma \sum_{s' \in \mathcal{S}} p(s'|s,a) v_\pi(s') \right], \quad s \in \mathcal{S}.

\end{align*}

$$

$v_\pi(s)$和$v_\pi(s')$需要计算的状态值 ，是未知量， $\pi(a|s)$一个给定的策略，是已知量。 $p(r|s,a)$和$p(s'|s,a)$代表系统模型，这是已知量/未知量

##### 第一个等价形式是

$$v_\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r|s, a) \left[ r + \gamma v_\pi(s') \right].$$

这实际上是文献[3]使用的表达式。这个式子可以通过将下面的全概率公式（参见附录A）代入式(2.9)得到：

$$p(s'|s, a) = \sum_{r \in \mathcal{R}} p(s', r|s, a),$$

$$p(r|s, a) = \sum_{s' \in \mathcal{S}} p(s', r|s, a).$$

##### 第二个常见的等价形式是贝尔曼期望方程（Bellman expectation equation）：

$$v_\pi(s) = \mathbb{E}\left[ R_{t+1} + \gamma v_\pi(S_{t+1}) \bigg| S_t = s \right], \quad s \in \mathcal{S}.$$

这是因为$\mathbb{E}[G_{t+1} | S_t = s] = \sum_a \pi(a|s) \sum_{s'} p(s'|s, a) v_\pi(s') = \mathbb{E}\left[ v_\pi(S_{t+1}) \bigg| S_t = s \right]$，将其代入(2.6)即可得到贝尔曼期望方程。

##### 第三，奖励$r$在某些问题中可能仅依赖于下一个状态$s'$，即奖励可以写成$r(s')$。此时我们有$p(r(s')|s, a) = p(s'|s, a)$，将其代入(2.9)可得另外一个表达式

$$v_\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{s' \in \mathcal{S}} p(s'|s, a) \left[ r(s') + \gamma v_\pi(s') \right].$$
#### 矩阵向量形式
$$ \begin{align*} v_\pi(s) &= r_\pi(s) + \gamma \sum_{s' \in \mathcal{S}} p_\pi(s'|s) v_\pi(s'), \\ r_\pi(s) &\triangleq \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{r \in \mathcal{R}} p(r|s, a) r, \\ p_\pi(s'|s) &\triangleq \sum_{a \in \mathcal{A}} \pi(a|s) p(s'|s, a). \end{align*} $$
$$v_\pi(s_i) = r_\pi(s_i) + \gamma \sum_{s_j \in \mathcal{S}} p_\pi(s_j|s_i) v_\pi(s_j). \tag{2.12}$$定义 $v_\pi = [v_\pi(s_1), \dots, v_\pi(s_n)]^\mathrm{T} \in \mathbb{R}^n$，$r_\pi = [r_\pi(s_1), \dots, r_\pi(s_n)]^\mathrm{T} \in \mathbb{R}^n$，以及 $P_\pi \in \mathbb{R}^{n \times n}$，其中 $P_\pi$ 满足 $[P_\pi]_{ij} = p_\pi(s_j|s_i)$。此时(2.12)可以用下列矩阵-向量形式来表示： $$v_\pi = r_\pi + \gamma P_\pi v_\pi, \tag{2.13}$$
$$

\underbrace{\begin{bmatrix}

v_\pi(s_1) \\

v_\pi(s_2) \\

v_\pi(s_3) \\

v_\pi(s_4)

\end{bmatrix}}_{v_\pi}

=

\underbrace{\begin{bmatrix}

r_\pi(s_1) \\

r_\pi(s_2) \\

r_\pi(s_3) \\

r_\pi(s_4)

\end{bmatrix}}_{r_\pi}

+

\gamma

\underbrace{\begin{bmatrix}

p_\pi(s_1|s_1) & p_\pi(s_2|s_1) & p_\pi(s_3|s_1) & p_\pi(s_4|s_1) \\

p_\pi(s_1|s_2) & p_\pi(s_2|s_2) & p_\pi(s_3|s_2) & p_\pi(s_4|s_2) \\

p_\pi(s_1|s_3) & p_\pi(s_2|s_3) & p_\pi(s_3|s_3) & p_\pi(s_4|s_3) \\

p_\pi(s_1|s_4) & p_\pi(s_2|s_4) & p_\pi(s_3|s_4) & p_\pi(s_4|s_4)

\end{bmatrix}}_{P_\pi}

\underbrace{\begin{bmatrix}

v_\pi(s_1) \\

v_\pi(s_2) \\

v_\pi(s_3) \\

v_\pi(s_4)

\end{bmatrix}}_{v_\pi}

$$
求解：

1、解析解

$$ v_\pi = (I-\gamma P_\pi)^{-1}r_\pi $$

2、迭代解

$$ v_{k+1} = r_\pi +\gamma P_\pi v_\pi, k = 0,1,2... $$

#### **动作值**

动作值被定义为在一个状态采取一个动作之后获得的回报的期望值。
$$ \underbrace{\mathbb{E}[G_t | S_t = s]}_{v_\pi(s)} = \sum_{a \in \mathcal{A}} \underbrace{\mathbb{E}[G_t | S_t = s, A_t = a]}_{q_\pi(s,a)} \pi(a|s). $$ 上式可简化为 $$ \begin{align*} v_\pi(s) &= \sum_{a \in \mathcal{A}} \pi(a|s) q_\pi(s,a) \\ &= \mathbb{E}_{A_t \sim \pi(s)} \left[ q_\pi(s, A_t) \right]. \end{align*} $$
状态值就是改状态对应的动作值的期望值
$$ v_\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \left[ \sum_{r \in \mathcal{R}} p(r|s,a) r + \gamma \sum_{s' \in \mathcal{S}} p(s'|s,a) v_\pi(s') \right]. $$ 将其与(2.16)进行比较，可得 $$ \begin{align*} q_\pi(s,a) &= \sum_{r \in \mathcal{R}} p(r|s,a) r + \gamma \sum_{s' \in \mathcal{S}} p(s'|s,a) v_\pi(s') \\ &= \mathbb{E} \left[ R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = a \right]. \end{align*} $$
动作值是一个包含状态值变量的期望值
#### 基于动作值的贝尔曼方程
$$ q_\pi(s,a) = \sum_{r \in \mathcal{R}} p(r|s,a) r + \gamma \sum_{s' \in \mathcal{S}} p(s'|s,a) \sum_{a' \in \mathcal{A}(s')} \pi(a'|s') q_\pi(s',a'). $$
$$q_\pi = \tilde{r} + \gamma P \Pi q_\pi, \tag{2.18}$$其中 $q_\pi$ 是一个动作值向量，它对应 $(s,a)$ 的元素是 $[q_\pi]_{(s,a)} = q_\pi(s,a)$；$\tilde{r}$ 是由 $(s,a)$ 索引的即时奖励向量 $[\tilde{r}]_{(s,a)} = \sum_{r \in \mathcal{R}} p(r|s,a) r$；矩阵 $P$ 是概率转移矩阵，其每一行对应一个状态-动作配对，每一列对应一个状态 $[P]_{(s,a),s'} = p(s'|s,a)$；$\Pi$ 是一个块对角矩阵（block diagonal matrix），其中每个块是一个 $1 \times |\mathcal{A}|$ 维的向量：$\Pi_{s',(s',a')} = \pi(a'|s')$，而 $\Pi$ 的其他元素都为0。

