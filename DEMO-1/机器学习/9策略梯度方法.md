---
date: 2025-11-03
---
基于值（1-8）$\rightarrow$基于策略
### 策略表示：由表格到函数
策略也可以用函数来表示，记为$\pi(a|s, \theta)$，其中$\theta \in \mathbb{R}^m$是参数向量。（$\pi_\theta(a|s)$、$\pi_\theta(a, s)$、$\pi(a, s, \theta)$）
#### 表格法和函数法之间的区别 
1. 定义最优策略的方式不同。 
	- 表格描述策略时，最优策略的定义是它能够最大化所有状态的状态值，即其状态值大于或等于其他任意策略的状态值。
	- 函数描述策略时，最优策略的定义是它能够最大化一个标量目标函数。 
2. 更新策略的方式不同。 
	- 表格描述策略时，可以通过直接改变表格中的元素来直接更新选择某些动作的概率。
	- 函数描述策略时，不能再以这种方式更新策略，而只能通过改变函数参数$\theta$来间接更新选择某些动作的概率。
3. 查看动作概率的方式不同。 
	- 表格描述策略时，可以通过查看表格中相应的元素直接获得某个动作的概率。
	- 函数描述策略时，我们需要将$(s, a)$输入到函数中，通过计算函数值来获得其概率。(所有状态)
![[9-1.png|925]]
当用函数表示策略时，我们的任务是**最大化一个标量目标函数**$J(\theta)$，其中$\theta$代表策略函数的参数。不同参数对应不同的目标函数值，因此我们需要找到最优的参数从而优化该目标函数。最简单的优化方法是梯度上升： $$ \theta_{t+1} = \theta_t + \alpha \nabla_\theta J(\theta_t), $$ 其中$\nabla_\theta J$是$J$相对于$\theta$的梯度，$\alpha > 0$是步长。
### 目标函数：定义最优策略
#### 目标函数1：平均状态值
 $$ \bar{v}_\pi = \sum_{s \in S} d(s) v_\pi(s), $$
 其中$d(s)$是状态$s$的权重，它满足对任何$s \in S$有$d(s) \geqslant 0$且$\sum_{s \in S} d(s) = 1$。权重$d(s)$也可以理解为状态$s$的概率分布，那么该目标函数可以重写为 $$ \bar{v}_\pi = \mathbb{E}_{S \sim d}[v_\pi(S)]. $$ 顾名思义，$\bar{v}_\pi$是所有状态值的加权平均。不同的$\theta$值将导致不同的$\bar{v}_\pi$值。
 选择概率分布$d(s)$
 1. $d$与策略$\pi$无关，此时该目标函数对策略参数求梯度不需要考虑$d$，我们特别地用$d_0$来代替$d$，用$\bar{v}_\pi^0$来代替$\bar{v}_\pi$，以表明该概率分布与策略无关。 例如，如果我们认为所有状态的重要性相同，那么可以选择$d_0(s) = 1/|S|$。如果我们只对某个特定状态$s_0$感兴趣（例如智能体始终从$s_0$出发），那么可以设计 $$ d_0(s_0) = 1, \quad d_0(s \neq s_0) = 0. $$ 此时$\bar{v}_\pi = v_\pi(s_0)$，优化该目标函数就是优化从$s_0$出发的回报期望值。
 2. $d$与策略$\pi$有关。此时常见的选择是将$d$设为$d_\pi$，即在$\pi$下的平稳分布。平稳分布反映了在给定策略下马尔可夫决策过程的长期行为。如果一个状态在长期内经常被访问，则其重要性高，应该有更高的权重；如果一个状态很少被访问，则其重要性低，应该有较低的权重。
$\bar{v}_\pi$的两个等价表达式。
- 等价表达式1：假设智能体根据给定策略$\pi(\theta)$收集了一个奖励序列$\{R_{t+1}\}_{t=0}^\infty$。 $$ J(\theta) = \lim_{n \to \infty} \mathbb{E}\left[ \sum_{t=0}^n \gamma^t R_{t+1} \right] = \mathbb{E}\left[ \sum_{t=0}^\infty \gamma^t R_{t+1} \right]. \tag{9.1} $$
- 等价表达式2：目标函数$\bar{v}_\pi$也可以重写为两个向量的内积。令 $$ v_\pi = [\dots, v_\pi(s), \dots]^\text{T} \in \mathbb{R}^{|\mathcal{S}|}, $$ $$ d = [\dots, d(s), \dots]^\text{T} \in \mathbb{R}^{|\mathcal{S}|}. $$ 那么有 $$ \bar{v}_\pi = d^\text{T} v_\pi. $$
#### 目标函数2：平均奖励 
$$ \begin{align*} \bar{r}_\pi &\doteq \sum_{s \in S} d_\pi(s) r_\pi(s) \\ &= \mathbb{E}_{S \sim d_\pi}\left[ r_\pi(S) \right], \tag{9.2} \end{align*} $$ 其中$d_\pi$是平稳分布，另外 $$ r_\pi(s) \doteq \sum_{a \in \mathcal{A}} \pi(a|s, \theta) r(s, a) = \mathbb{E}_{A \sim \pi(s, \theta)}\left[ r(s, A) \bigg| s \right] \tag{9.3} $$ 是从状态$s$出发的（单步）即时奖励的期望值。这里$r(s, a) \doteq \mathbb{E}[R|s, a] = \sum_r r p(r|s, a)$。 
$\bar{r}_\pi$的两个等价表达式。
- 等价表达式1：假设智能体根据给定策略$\pi(\theta)$收集到一个奖励序列$\{R_{t+1}\}_{t=0}^\infty$。大家可能经常在文献中看到如下目标函数： $$ J(\theta) = \lim_{n \to \infty} \frac{1}{n} \mathbb{E}\left[ \sum_{t=0}^{n-1} R_{t+1} \right]. \tag{9.4} $$
- 等价表达式2：平均奖励$\bar{r}_\pi$也可以表示为两个向量的内积。令 $$ r_\pi = \left[ \dots, r_\pi(s), \dots \right]^{\mathrm{T}} \in \mathbb{R}^{|S|}, $$$$ d_\pi = \left[ \dots, d_\pi(s), \dots \right]^{\mathrm{T}} \in \mathbb{R}^{|S|}, $$$$ \bar{r}_\pi = \sum_{s \in S} d_\pi(s) r_\pi(s) = d_\pi^{\mathrm{T}} r_\pi. $$
#### 总结

| 目标函数          | 表达式1                               | 表达式2                                  | 表达式3                                                                                  |
| ------------- | ---------------------------------- | ------------------------------------- | ------------------------------------------------------------------------------------- |
| $\bar{v}_\pi$ | $\sum_{s \in S} d(s) v_\pi(s)$     | $\mathbb{E}_{S \sim d}[v_\pi(S)]$     | $\lim_{n \to \infty} \mathbb{E}\left[ \sum_{t=0}^n \gamma^t R_{t+1} \right]$          |
| $\bar{r}_\pi$ | $\sum_{s \in S} d_\pi(s) r_\pi(s)$ | $\mathbb{E}_{S \sim d_\pi}[r_\pi(S)]$ |  $\lim_{n \to \infty} \frac{1}{n} \mathbb{E}\left[ \sum_{t=0}^{n-1} R_{t+1} \right]$  |

 - 两个目标函数$\bar{v}_\pi$（平均状态值）和$\bar{r}_\pi$（平均奖励）在$\gamma < 1$时是等价的，满足$\bar{r}_\pi = (1 - \gamma)\bar{v}_\pi$，因此二者可同时最大化。
 - 它们都是策略$\pi$的函数，而$\pi$由参数$\theta$化，所以目标函数最终是$\theta$的函数，策略梯度方法的核心就是寻找最优$\theta$来最大化这些目标函数。
### 目标函数的梯度
**定理9.1 (策略梯度定理)**。$J(\theta)$的梯度是 $$ \nabla_\theta J(\theta) = \sum_{s \in S} \eta(s) \sum_{a \in \mathcal{A}} \nabla_\theta \pi(a|s, \theta) q_\pi(s, a), \tag{9.8} $$ 其中$\eta$是状态的概率分布，$\nabla_\theta \pi$是$\pi$关于$\theta$的梯度。此外，式(9.8)有如下等价的形式： $$ \nabla_\theta J(\theta) = \mathbb{E}_{S \sim \eta, A \sim \pi(S, \theta)}\left[ \nabla_\theta \ln \pi(A|S, \theta) q_\pi(S, A) \right], \tag{9.9} $$ 其中$\ln$是自然对数。
***
自然对数$\ln$要求$\pi(a|s,\theta)$对所有$(s,a)$都满足$\pi(a|s,\theta) > 0$（而不能出现$\pi(a|s,\theta) = 0$），因此这个策略必须是随机且探索性的，这可以通过使用Softmax函数来实现： $$ \pi(a|s, \theta) = \frac{e^{h(s,a,\theta)}}{\sum_{a' \in \mathcal{A}} e^{h(s,a',\theta)}}, \quad a \in \mathcal{A}, \tag{9.12} $$ 其中$h(s,a,\theta)$是一个特征函数，表示在状态$s$选择动作$a$的优先度。式(9.12)中的策略满足$\pi(a|s,\theta) \in (0,1)$并且$\sum_{a \in \mathcal{A}} \pi(a|s,\theta) = 1$对任何$s \in S$都成立。这个策略可以通过神经网络实现：网络的输入是$s$，输出层是一个Softmax层，因此网络输出所有动作的概率为$\pi(a|s,\theta)$，并且输出的总和等于1。
***
#### 有折扣情况
**引理9.1 ($\bar{v}_\pi(\theta)$与$\bar{r}_\pi(\theta)$等价)**。在有折扣的情况下，即当$\gamma \in (0,1)$时，有 $$ \bar{r}_\pi = (1 - \gamma)\bar{v}_\pi. \tag{9.13} $$ 因此，$\bar{v}_\pi(\theta)$和$\bar{r}_\pi(\theta)$可以被同时最大化。 
**证明**：注意到$\bar{v}_\pi(\theta) = d_\pi^{\mathrm{T}} v_\pi$并且$\bar{r}_\pi(\theta) = d_\pi^{\mathrm{T}} r_\pi$，其中$v_\pi, r_\pi$满足贝尔曼方程$v_\pi = r_\pi + \gamma P_\pi v_\pi$。在贝尔曼方程两边同乘以$d_\pi^{\mathrm{T}}$可得 $$ \bar{v}_\pi = \bar{r}_\pi + \gamma d_\pi^{\mathrm{T}} P_\pi v_\pi = \bar{r}_\pi + \gamma d_\pi^{\mathrm{T}} v_\pi = \bar{r}_\pi + \gamma \bar{v}_\pi. $$
**引理9.2 (状态值的梯度)**。在有折扣的情况下，即当$\gamma \in (0,1)$时，对于任意$s \in S$都有 $$ \nabla_\theta v_\pi(s) = \sum_{s' \in S} \mathrm{Pr}_\pi(s'(s) \sum_{a \in \mathcal{A}} \nabla_\theta \pi(a|s', \theta) q_\pi(s', a), \tag{9.14} $$ 其中 $$ \mathrm{Pr}_\pi(s'(s) = \sum_{k=0}^\infty \gamma^k [P_\pi^k]_{ss'} = [(I_n - \gamma P_\pi)^{-1}]_{ss'} $$ 是在策略$\pi$下从状态$s$转移到状态$s'$的折扣总概率。这里$[\cdot]_{ss'}$表示矩阵的第$s$行和第$s'$列的元素。$[P_\pi^k]_{ss'}$等于在策略$\pi$下恰好用$k$步从$s$转移到$s'$的概率。
“0”表示该目标函数的转台概率分布与策略$\pi$无关
**定理9.2 (有折扣的情况下$\bar{v}_\pi^0$的梯度)**。在有折扣的情况下，即当$\gamma \in (0,1)$时，$\bar{v}_\pi^0 = d_0^{\mathrm{T}} v_\pi$的梯度是 $$ \nabla_\theta \bar{v}_\pi^0 = \mathbb{E}\left[ \nabla_\theta \ln \pi(A|S, \theta) q_\pi(S, A) \right], $$ 其中$S \sim \rho_\pi$，$A \sim \pi(S, \theta)$而且 $$ \rho_\pi(s) = \sum_{s' \in S} d_0(s') \mathrm{Pr}_\pi(s|s'), \quad s \in S, \tag{9.19} $$ 其中$\mathrm{Pr}_\pi(s|s') = \sum_{k=0}^\infty \gamma^k [P_\pi^k]_{s's} = [(I - \gamma P_\pi)^{-1}]_{s's}$是在策略$\pi$下从$s'$到$s$的折扣总概率。
**定理9.3 (有折扣的情况下$\bar{v}_\pi$和$\bar{r}_\pi$的梯度)**。在有折扣的情况下，即当$\gamma \in (0,1)$时，$\bar{v}_\pi$和$\bar{r}_\pi$的梯度为 $$ \begin{align*} \nabla_\theta \bar{r}_\pi &= (1 - \gamma)\nabla_\theta \bar{v}_\pi \approx \sum_{s \in S} d_\pi(s) \sum_{a \in \mathcal{A}} \nabla_\theta \pi(a|s, \theta) q_\pi(s, a) \\ &= \mathbb{E}\left[ \nabla_\theta \ln \pi(A|S, \theta) q_\pi(S, A) \right], \end{align*} $$ 其中$S \sim d_\pi$，$A \sim \pi(S, \theta)$。当$\gamma$接近1时，上面的近似更加准确。
#### 无折扣情况
##### 状态值和泊松方程
状态值和动作值新定义： $$ v_\pi(s) \doteq \mathbb{E}\left[(R_{t+1} - \bar{r}_\pi) + (R_{t+2} - \bar{r}_\pi) + (R_{t+3} - \bar{r}_\pi) + \dots \bigg| S_t = s\right], $$ $$ q_\pi(s, a) \doteq \mathbb{E}\left[(R_{t+1} - \bar{r}_\pi) + (R_{t+2} - \bar{r}_\pi) + (R_{t+3} - \bar{r}_\pi) + \dots \bigg| S_t = s, A_t = a\right], $$ 其中$\bar{r}_\pi$是平均奖励。$v_\pi(s)$（差分奖励（differential reward）或偏置（bias）） $$ v_\pi(s) = \sum_a \pi(a|s, \theta) \left[ \sum_r p(r|s, a)(r - \bar{r}_\pi) + \sum_{s'} p(s'|s, a) v_\pi(s') \right]. \tag{9.22} $$动作值的表达式为$q_\pi(s, a) = \sum_r p(r|s, a)(r - \bar{r}_\pi) + \sum_{s'} p(s'|s, a) v_\pi(s')$。
矩阵-向量形式$$ v_\pi = r_\pi - \bar{r}_\pi \mathbf{1}_n + P_\pi v_\pi, \tag{9.23} $$ 其中$\mathbf{1}_n = [1, \dots, 1]^{\mathrm{T}} \in \mathbb{R}^n$。方程(9.22)和(9.23)与贝尔曼方程很类似：**泊松方程（Poisson equation）**
**定理9.4 (泊松方程的解)**。令 $$ v_\pi^* \doteq (I_n - P_\pi + \mathbf{1}_n d_\pi^{\mathrm{T}})^{-1} r_\pi. \tag{9.24} $$ 那么$v_\pi^*$是式(9.23)中泊松方程的一个解，且泊松方程的任意解具有以下形式： $$ v_\pi = v_\pi^* + c\mathbf{1}_n, $$ 其中$c \in \mathbb{R}$。 上述定理表明泊松方程的解可能是不唯一的。
##### 梯度的推导
$\bar{r}_\pi$的值是唯一的。具体来说，将$v_\pi = v_\pi^* + c\mathbf{1}_n$代入泊松方程可得 $$ \begin{align*} \bar{r}_\pi \mathbf{1}_n &= r_\pi + (P_\pi - I_n)v_\pi \\ &= r_\pi + (P_\pi - I_n)(v_\pi^* + c\mathbf{1}_n) \\ &= r_\pi + (P_\pi - I_n)v_\pi^*. \end{align*} $$ 注意其中$c$被抵消了，因此$\bar{r}_\pi$的值是唯一的，所以我们可以在无折扣的情况下计算$\bar{r}_\pi$的梯度。
**定理9.5 (无折扣情况下$\bar{r}_\pi$的梯度)**。在无折扣的情况下，平均奖励$\bar{r}_\pi$的梯度是 $$ \begin{align*} \nabla_\theta \bar{r}_\pi &= \sum_{s \in S} d_\pi(s) \sum_{a \in \mathcal{A}} \nabla_\theta \pi(a|s, \theta) q_\pi(s, a) \\ &= \mathbb{E}\left[ \nabla_\theta \ln \pi(A|S, \theta) q_\pi(S, A) \right], \tag{9.28} \end{align*} $$ 其中$S \sim d_\pi$，$A \sim \pi(S, \theta)$。 与前面有折扣的情况下的结果相比（定理9.3），$\bar{r}_\pi$在无折扣的情况下的梯度在数学上更为优美，这是因为式(9.28)是严格成立的。
### 蒙托卡罗策略梯度（REINFORCE)
利用如下的梯度上升算法来最大化目标函数以获得最佳策略： $$ \begin{align*} \theta_{t+1} &= \theta_t + \alpha \nabla_\theta J(\theta_t) \\ &= \theta_t + \alpha \mathbb{E}\left[ \nabla_\theta \ln \pi(A|S, \theta_t) q_\pi(S, A) \right], \tag{9.31} \end{align*} $$ 其中$\alpha > 0$是学习率。
我们可以用随机梯度替换真实梯度，从而得到如下算法： $$ \theta_{t+1} = \theta_t + \alpha \nabla_\theta \ln \pi(a_t|s_t, \theta_t) q_t(s_t, a_t), \tag{9.32} $$ 其中$q_t(s_t, a_t)$是对$q_\pi(s_t, a_t)$在$t$时刻的估计值。
策略参数$\theta_t$的更新依赖于对动作值的估计$q_t(s_t, a_t)$。
两种估计值的方法，一种是蒙特卡罗方法，另一种是时序差分方法。如果$q_t(s_t, a_t)$是通过蒙特卡罗估计得到的，那么该算法被称为蒙特卡罗策略梯度（Monte Carlo policy gradient）或者REINFORCE。如果$q_t(s_t, a_t)$是通过时序差分方法得到的，那么相应的算法实际上就是Actor-Critic方法，(9.32)。由于 $$ \nabla_\theta \ln \pi(a_t|s_t, \theta_t) = \frac{\nabla_\theta \pi(a_t|s_t, \theta_t)}{\pi(a_t|s_t, \theta_t)}, $$
算法(9.32)可重写为 $$ \theta_{t+1} = \theta_t + \alpha \underbrace{\left( \frac{q_t(s_t, a_t)}{\pi(a_t|s_t, \theta_t)} \right)}_{\beta_t} \nabla_\theta \pi(a_t|s_t, \theta_t). $$ 上式可简写为 $$ \theta_{t+1} = \theta_t + \alpha \beta_t \nabla_\theta \pi(a_t|s_t, \theta_t). \tag{9.33} $$两方面的重要结论。
1. 如果$\beta_t \geqslant 0$，则在$s_t$选择$a_t$的概率会增大，即 $$ \pi(a_t|s_t, \theta_{t+1}) \geqslant \pi(a_t|s_t, \theta_t). $$
   如果$\beta_t < 0$，则在$s_t$选择$a_t$的概率会降低，即 $$ \pi(a_t|s_t, \theta_{t+1}) < \pi(a_t|s_t, \theta_t). $$ 当$\theta_{t+1} - \theta_t$足够小时，根据一阶泰勒展开可知 $$ \begin{align*} \pi(a_t|s_t, \theta_{t+1}) &\approx \pi(a_t|s_t, \theta_t) + (\nabla_\theta \pi(a_t|s_t, \theta_t))^{\mathrm{T}} (\theta_{t+1} - \theta_t) \\ &= \pi(a_t|s_t, \theta_t) + \alpha \beta_t (\nabla_\theta \pi(a_t|s_t, \theta_t))^{\mathrm{T}} (\nabla_\theta \pi(a_t|s_t, \theta_t)) \quad \text{（代入(9.33)）} \\ &= \pi(a_t|s_t, \theta_t) + \alpha \beta_t \|\nabla_\theta \pi(a_t|s_t, \theta_t)\|_2^2. \end{align*} $$ 很明显，当$\beta_t \geqslant 0$时，$\pi(a_t|s_t, \theta_{t+1}) \geqslant \pi(a_t|s_t, \theta_t)$；当$\beta_t < 0$时，$\pi(a_t|s_t, \theta_{t+1}) < \pi(a_t|s_t, \theta_t)$。 
 2. 根据上述第一个结论和$\beta_t$的表达式，我们可以知道该算法可以平衡探索（exploration）和利用（exploitation）。注意$\beta_t$的表达式为 $$ \beta_t = \frac{q_t(s_t, a_t)}{\pi(a_t|s_t, \theta_t)}. $$ 一方面，$\beta_t$与$q_t(s_t, a_t)$呈正比。如果$q_t(s_t, a_t)$较大，那么$\pi(a_t|s_t, \theta_t)$将增大，即下一个时刻选择$a_t$的概率会增大，因此该算法倾向于利用具有更大价值的动作。另一方面，当$q_t(s_t, a_t) > 0$时，$\beta_t$与$\pi(a_t|s_t, \theta_t)$呈反比。此时，如果$\pi(a_t|s_t, \theta_t)$较小，即选择$a_t$的概率较小，那么$\pi(a_t|s_t, \theta_t)$将增大，即下一个时刻选择$a_t$的概率会增大，因此该算法会探索那些之前概率低的动作。
 随机采样
 - 第一，采样$S$：真实梯度$\mathbb{E}\left[ \nabla_\theta \ln \pi(A|S, \theta) q_\pi(S, A) \right]$中的$S$应服从概率分布$\eta$，这是平稳分布$d_\pi$或者式(9.19)给出的分布$\rho_\pi$。无论是哪一个分布，都代表在策略$\pi$下的长期行为。 
 - 第二，采样$A$：真实梯度$\mathbb{E}\left[ \nabla_\theta \ln \pi(A|S, \theta_t) q_\pi(S, A) \right]$中的$A$应服从概率分布$\pi(A|S, \theta)$。采样$A$的理想方式是按照$\pi(a|s_t, \theta_t)$采样得到$a_t$。
###### 伪代码
***
初始化：初始参数$\theta$；$\gamma \in (0,1)$；$\alpha > 0$。 
目标：学习一个最优策略从而最大化$J(\theta)$。 
- 对于每个回合 
	- 根据$\pi(\theta)$生成$\{s_0, a_0, r_1, \dots, s_{T-1}, a_{T-1}, r_T\}$。 
	- 对于$t = 0,1,\dots,T-1$： 
		- 价值更新：$q_t(s_t, a_t) = \sum_{k=t+1}^T \gamma^{k-t-1} r_k$ 
		- 策略更新：$\theta \leftarrow \theta + \alpha \nabla_\theta \ln \pi(a_t|s_t, \theta) q_t(s_t, a_t)$
***
