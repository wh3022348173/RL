---
aliases:
date: 2025-11-05
---
融合了基于策略和基于价值的两类方法。
“Actor”对应的是策略更新r，它对应生成动作的策略。
“Critic”指的是价值更新，它会评估策略相应的价值。
### 最简单的演员-评论家算法：QAC
用于最大化$J(\theta)$的梯度上升算法是 $$ \begin{align*} \theta_{t+1} &= \theta_t + \alpha \nabla_\theta J(\theta_t) \\ &= \theta_t + \alpha \mathbb{E}_{S \sim \eta, A \sim \pi}\left[ \nabla_\theta \ln \pi(A|S, \theta_t) q_\pi(S, A) \right], \tag{10.1} \end{align*} $$ 其中$\eta$是状态的分布。由于真实的梯度是无法得到的，我们可以使用随机梯度来近似： $$ \theta_{t+1} = \theta_t + \alpha \nabla_\theta \ln \pi(a_t|s_t, \theta_t) q_t(s_t, a_t). \tag{10.2} $$式(10.2)清楚地展示了如何融合基于策略的方法和基于价值的方法。一方面，它是一个基于策略的算法，因为它直接更新策略参数。另一方面，它的更新需要知道$q_t(s_t, a_t)$，这是动作值$q_\pi(s_t, a_t)$的估计量，需要另一个基于价值的算法来得到$q_t(s_t, a_t)$。 
###### 伪代码
***
初始化：一个策略函数$\pi(a|s, \theta_0)$，其中$\theta_0$是初始参数。一个价值函数$q(s, a, w_0)$，其中$w_0$是初始参数。$\alpha_w, \alpha_\theta > 0$。 
目标：学习一个最优策略来最大化$J(\theta)$。 
- 在每个回合中的$t$时刻 
	- 根据$\pi(a|s_t, \theta_t)$产生$a_t$，观测$r_{t+1}, s_{t+1}$，然后根据$\pi(a|s_{t+1}, \theta_t)$生成$a_{t+1}$ 
	- Actor（策略更新）： $$ \theta_{t+1} = \theta_t + \alpha_\theta \nabla_\theta \ln \pi(a_t|s_t, \theta_t) q(s_t, a_t, w_t) $$
	- Critic（价值更新）： $$ w_{t+1} = w_t + \alpha_w \left[ r_{t+1} + \gamma q(s_{t+1}, a_{t+1}, w_t) - q(s_t, a_t, w_t) \right] \nabla_w q(s_t, a_t, w_t) $$
***
### 优势演员 - 评论家（advantage actor-critic，A2C）
这个算法的核心思想是引入一个基准来减少估计的方差。
#### 基准不变性
它对额外的基准（baseline）是不变的，即 $$ \mathbb{E}_{S \sim \eta, A \sim \pi}\left[ \nabla_\theta \ln \pi(A|S, \theta_t) q_\pi(S, A) \right] = \mathbb{E}_{S \sim \eta, A \sim \pi}\left[ \nabla_\theta \ln \pi(A|S, \theta_t)(q_\pi(S, A) - b(S)) \right], \tag{10.3} $$ 其中$b(S)$是基准函数，它是$S$的一个标量函数。上式表明了添加或去掉基准函数$b(S)$不会影响策略梯度。
- 充分必要条件
$$ \mathbb{E}_{S \sim \eta, A \sim \pi}\left[ \nabla_\theta \ln \pi(A|S, \theta_t) b(S) \right] = 0. $$
- 基准函数之所以有用，是因为它能够在我们使用随机样本近似真实梯度时减少近似的方差。具体来说，定义 $$ X(S, A) \doteq \nabla_\theta \ln \pi(A|S, \theta_t)(q_\pi(S, A) - b(S)). \tag{10.4} $$ 此时真实的梯度是$\mathbb{E}[X(S, A)]$。
  能够最小化$\text{var}(X)$的最优基准是 $$ b^*(s) = \frac{\mathbb{E}_{A \sim \pi}\left[ \|\nabla_\theta \ln \pi(A|s, \theta_t)\|^2 q_\pi(s, A) \right]}{\mathbb{E}_{A \sim \pi}\left[ \|\nabla_\theta \ln \pi(A|s, \theta_t)\|^2 \right]}, \quad s \in S. \tag{10.5} $$ 但它太复杂，无法在实际中使用。如果从式(10.5)中移除权重$\|\nabla_\theta \ln \pi(A|s, \theta_t)\|^2$，就可以得到一个次优的基准，它有一个简洁的表达式： $$ b^\dagger(s) = \mathbb{E}_{A \sim \pi}[q_\pi(s, A)] = v_\pi(s), \quad s \in S. $$ 值得注意的是，这个次优的基准函数就是状态值函数。
#### 算法描述
当$b(s) = v_\pi(s)$时，梯度上升算法变成了 $$ \begin{align*} \theta_{t+1} &= \theta_t + \alpha \mathbb{E}\left[ \nabla_\theta \ln \pi(A|S, \theta_t)(q_\pi(S, A) - v_\pi(S)) \right] \\ &\doteq \theta_t + \alpha \mathbb{E}\left[ \nabla_\theta \ln \pi(A|S, \theta_t)\delta_\pi(S, A) \right]. \tag{10.7} \end{align*} $$ 其中 $$ \delta_\pi(S, A) \doteq q_\pi(S, A) - v_\pi(S) $$ 被称为**优势函数（advantage function）**，它反映了一个动作相对于其他动作的优势。具体来说，由于状态值$v_\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s, \theta) q_\pi(s, a)$是平均动作值，因此$\delta_\pi(s, a) > 0$意味着相应的动作值大于均值，具有一定的优势。 
如果把式(10.7)中的真实梯度替换成随机梯度，可以得到 $$ \begin{align*} \theta_{t+1} &= \theta_t + \alpha \nabla_\theta \ln \pi(a_t|s_t, \theta_t)[q_t(s_t, a_t) - v_t(s_t)] \\ &= \theta_t + \alpha \nabla_\theta \ln \pi(a_t|s_t, \theta_t)\delta_t(s_t, a_t). \tag{10.8} \end{align*} $$ 其中$s_t, a_t$是在$t$时刻$S, A$的样本。这里$q_t(s_t, a_t)$和$v_t(s_t)$分别是$q_\pi(s_t, a_t)$和$v_\pi(s_t)$的估计值。算法(10.8)是基于$q_t - v_t$这个相对值更新策略的，而不是基于其绝对值。
如果$q_t(s_t, a_t)$和$v_t(s_t)$是通过蒙特卡罗方法估计的，那么式(10.8)中的算法被称为**带基准的REINFORCE（REINFORCE with baseline）**。如果$q_t(s_t, a_t)$和$v_t(s_t)$是通过时序差分方法估计的，那么这种算法通常被称为**Advantage Actor-Critic（A2C）**。
###### 伪代码（**Advantage Actor-Critic（A2C）**）
***
初始化：策略函数$\pi(a|s, \theta_0)$，其中$\theta_0$是初始参数。价值函数$v(s, w_0)$，其中$w_0$是初始参数。$\alpha_w, \alpha_\theta > 0$。 
目标：学习最优策略以最大化$J(\theta)$。 
- 在每个回合中的$t$时刻 
	- 根据$\pi(a|s_t, \theta_t)$生成$a_t$，然后得到$r_{t+1}, s_{t+1}$ 
	- 优势函数（时序差分误差）： $$ \delta_t = r_{t+1} + \gamma v(s_{t+1}, w_t) - v(s_t, w_t) $$ 
	- Actor（策略更新）： $$ \theta_{t+1} = \theta_t + \alpha_\theta \delta_t \nabla_\theta \ln \pi(a_t|s_t, \theta_t) $$ 
	- Critic（价值更新）： $$ w_{t+1} = w_t + \alpha_w \delta_t \nabla_w v(s_t, w_t) $$
***
算法给出了A2C算法的流程。优势函数是通过时序差分误差近似的，即 $$ q_t(s_t, a_t) - v_t(s_t) \approx r_{t+1} + \gamma v_t(s_{t+1}) - v_t(s_t). $$ 这个近似是合理的原因是 $$ q_\pi(s_t, a_t) - v_\pi(s_t) = \mathbb{E}\left[ R_{t+1} + \gamma v_\pi(S_{t+1}) - v_\pi(S_t) \bigg| S_t = s_t, A_t = a_t \right]. $$使用时序差分误差的一个优势是我们只需要使用一个神经网络来表征$v_\pi(s)$。相反，如果我们使用$\delta_t = q_t(s_t, a_t) - v_t(s_t)$，则需要维护两个网络来分别表示$v_\pi(s)$和$q_\pi(s, a)$。当我们使用时序差分误差时，该算法也被称为**TD Actor-Critic**。此外，值得注意的是，$\pi(\theta_t)$是一个随机策略，因此它具有一定的探索性，所以它可以直接用来生成经验样本，而不需要诸如$\epsilon$-Greedy之类的技巧。A2C还有一些变体，例如A3C（asynchronous advantage actor-critic）等。
### 异策略演员-评论家
策略梯度方法，包括REINFORCE、QAC、A2C都是同策略（on-policy）的： $$ \nabla_\theta J(\theta) = \mathbb{E}_{S \sim \eta, A \sim \pi}\left[ \nabla_\theta \ln \pi(A|S, \theta_t)(q_\pi(S, A) - v_\pi(S)) \right]. $$ 为了使用随机梯度来近似这个真实梯度，我们必须按照$\pi(\theta)$生成动作样本。因此，$\pi(\theta)$是行为策略。因为$\pi(\theta)$也是我们要改进的目标策略，所以策略梯度方法是On-policy的。 如果我们已经有一些由其他行为策略生成的样本，那么策略梯度方法仍然可以使用这些样本来得到最优策略，此时的方法就变成了异策略（off-policy），不过此时需要采用一种称为重要性采样（importance sampling）的技术。
#### 1重要性采样 
考虑一个随机变量$X \in \mathcal{X}$。假设$p_0(X)$是一个概率分布，我们的目标是估计$\mathbb{E}_{X \sim p_0}[X]$。假设我们有一些独立同分布的样本$\{x_i\}_{i=1}^n$。
- 第一个场景：样本$\{x_i\}_{i=1}^n$是根据$p_0$生成的。此时，平均值$\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$可以用来近似$\mathbb{E}_{X\sim p_0}[X]$。这是因为$\bar{x}$是$\mathbb{E}_{X\sim p_0}[X]$的无偏估计，并且估计的方差随着$n\to\infty$收敛到0。
- 第二个场景：样本$\{x_i\}_{i=1}^n$不是根据$p_0$生成的，而是根据另一个概率分布$p_1$生成的。我们不能再使用$\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$来近似$\mathbb{E}_{X\sim p_0}[X]$，这是因为$\bar{x} \approx \mathbb{E}_{X\sim p_1}[X]$而非$\mathbb{E}_{X\sim p_0}[X]$。 我们需要使用**重要性采样**的技术来估计$\mathbb{E}_{X\sim p_0}[X]$。$$ \mathbb{E}_{X\sim p_0}[X] = \sum_{x\in\mathcal{X}} p_0(x) x = \sum_{x\in\mathcal{X}} p_1(x) \underbrace{\frac{p_0(x)}{p_1(x)} x}_{f(x)} = \mathbb{E}_{X\sim p_1}[f(X)]. \tag{10.9} $$令 $$ \bar{f} \doteq \frac{1}{n}\sum_{i=1}^n f(x_i). $$ 因为$\bar{f}$可以有效地近似$\mathbb{E}_{X\sim p_1}[f(X)]$，所以由式(10.9)可知 $$ \mathbb{E}_{X\sim p_0}[X] = \mathbb{E}_{X\sim p_1}[f(X)] \approx \bar{f} = \frac{1}{n}\sum_{i=1}^n f(x_i) = \frac{1}{n}\sum_{i=1}^n \underbrace{\frac{p_0(x_i)}{p_1(x_i)}}_{\text{重要性权重}} x_i. \tag{10.10} $$ 式(10.10)表明$\mathbb{E}_{X\sim p_0}[X]$可以通过$x_i$的加权平均来近似，而这里的权重就是$\frac{p_0(x_i)}{p_1(x_i)}$，它被称为**重要性权重**（importance weight）。
#### Off-policy策略梯度定理 
假设$\beta$是一个行为策略，我们的目标是使用由$\beta$生成的样本来得到一个目标策略$\pi$，从而最大化下面的目标函数： $$ J(\theta) = \sum_{s\in\mathcal{S}} d_\beta(s) v_\pi(s) = \mathbb{E}_{S\sim d_\beta}[v_\pi(S)], $$ 其中$d_\beta$是在策略$\beta$下的平稳分布，$v_\pi$是在策略$\pi$下的状态值。
**定理10.1（Off-policy策略梯度定理）**。如果$\gamma \in (0,1)$，那么$J(\theta)$的Off-policy梯度为 $$ \nabla_\theta J(\theta) = \mathbb{E}_{S\sim\rho, A\sim\beta}\left[ \underbrace{\frac{\pi(A|S,\theta)}{\beta(A|S)}}_{\text{重要性权重}} \nabla_\theta \ln \pi(A|S,\theta) q_\pi(S, A) \right], \tag{10.11} $$ 其中状态分布$\rho$为 $$ \rho(s) \doteq \sum_{s'\in\mathcal{S}} d_\beta(s') \text{Pr}_\pi(s|s'), \quad s \in \mathcal{S}. $$ 这里$\text{Pr}_\pi(s|s') = \sum_{k=0}^\infty \gamma^k [P_\pi^k]_{s's} = \left[(I - \gamma P_\pi)^{-1}\right]_{s's}$是在策略$\pi$下从$s'$到$s$的折扣总概率。
##### 基于重要性采样的Off-policy Actor-Critic算法
1. Off-policy策略梯度对额外的基准函数$b(s)$也是不变的。具体来说，因为$\mathbb{E}\left[ \frac{\pi(A|S,\theta)}{\beta(A|S)} \nabla_\theta \ln \pi(A|S,\theta) b(S) \right] = 0$，我们有 $$ \nabla_\theta J(\theta) = \mathbb{E}_{S\sim\rho, A\sim\beta}\left[ \frac{\pi(A|S,\theta)}{\beta(A|S)} \nabla_\theta \ln \pi(A|S,\theta) \big(q_\pi(S,A) - b(S)\big) \right]. $$
2. 为了降低估计方差，我们可以选择基准函数为$b(S) = v_\pi(S)$。此时策略梯度为 $$ \nabla_\theta J(\theta) = \mathbb{E}\left[ \frac{\pi(A|S,\theta)}{\beta(A|S)} \nabla_\theta \ln \pi(A|S,\theta) \big(q_\pi(S,A) - v_\pi(S)\big) \right]. $$ 
3. 对应的随机梯度算法是 $$ \theta_{t+1} = \theta_t + \alpha_\theta \frac{\pi(a_t|s_t,\theta)}{\beta(a_t|s_t)} \nabla_\theta \ln \pi(a_t|s_t,\theta) \big(q_t(s_t,a_t) - v_t(s_t)\big), $$ 其中$\alpha_\theta > 0$。
4. 优势函数$q_t(s_t,a) - v_t(s)$可以被时序差分误差所替代，即 $$ q_t(s_t,a_t) - v_t(s_t) \approx r_{t+1} + \gamma v_t(s_{t+1}) - v_t(s_t) \doteq \delta_t(s_t,a_t). $$ 此时，该算法变成了 $$ \theta_{t+1} = \theta_t + \alpha_\theta \frac{\pi(a_t|s_t,\theta)}{\beta(a_t|s_t)} \nabla_\theta \ln \pi(a_t|s_t,\theta) \delta_t(s_t,a_t). $$
###### 伪代码
***
初始化：给定一个行为策略$\beta(a|s)$。一个目标策略$\pi(a|s,\theta_0)$，其中$\theta_0$是初始参数。一个值函数$v(s,w_0)$，其中$w_0$是初始参数。$\alpha_w,\alpha_\theta > 0$。 
目标：学习一个最优策略以最大化$J(\theta)$。 
- 在每个回合中的$t$时刻 
	- 按照$\beta(s_t)$生成$a_t$，然后得到$r_{t+1},s_{t+1}$ - 优势函数（时序差分误差）： $$ \delta_t = r_{t+1} + \gamma v(s_{t+1}, w_t) - v(s_t, w_t) $$ 
	- Actor（策略更新）： $$ \theta_{t+1} = \theta_t + \alpha_\theta \frac{\pi(a_t|s_t,\theta_t)}{\beta(a_t|s_t)} \nabla_\theta \ln \pi(a_t|s_t,\theta_t) \delta_t $$ 
	- Critic（值更新）： $$ w_{t+1} = w_t + \alpha_w \frac{\pi(a_t|s_t,\theta_t)}{\beta(a_t|s_t)} \delta_t \nabla_w v(s_t, w_t) $$
***
### 确定性演员-评论家
“确定性”指的是对于任何一个状态，策略选择某一个动作的概率是1，而选择其他动作的概率都是0。 基于确定性策略的Actor-Critic方法被称为确定性Actor-Critic（deterministic actor-critic）或者确定性策略梯度（deterministic policy gradient）。
使用 $$ a = \mu(s,\theta) $$ 来专门表示一个确定性的策略。$\mu$是从$\mathcal{S}$到$\mathcal{A}$的一个映射，因此会直接输出一个动作。这与之前的$\pi$不同：$\pi$输出的是某一个动作的概率。
#### 确定性策略梯度定理
**定理10.2（确定性策略梯度定理）**。$J(\theta)$ 的梯度是 $$ \begin{align*} \nabla_\theta J(\theta) &= \sum_{s\in\mathcal{S}} \eta(s) \nabla_\theta \mu(s) \left.(\nabla_a q_\mu(s,a))\right|_{a=\mu(s)} \\ &= \mathbb{E}_{S\sim\eta} \left[ \nabla_\theta \mu(S) \left.(\nabla_a q_\mu(S,a))\right|_{a=\mu(S)} \right], \tag{10.14} \end{align*} $$ 其中 $\eta$ 是状态的分布。
##### 目标函数1：平均状态值 
平均状态值的表达式是 $$ J(\theta) = \mathbb{E}[v_\mu(s)] = \sum_{s\in\mathcal{S}} d_0(s) v_\mu(s), \tag{10.15} $$ 其中$d_0$是状态的概率分布。简单起见，我们可以假设$d_0$是一个与策略$\mu$独立的分布，这样$d_0$对$\theta$的梯度等于0。$d_0$的选择有两种特殊但重要的情形。
第一种情形是选择$d_0(s_0) = 1$且$d_0(s \neq s_0) = 0$，其中$s_0$是一个我们感兴趣的特定状态。在这种情况下，学习到的策略旨在最大化从$s_0$出发获得的回报。第二种情形是选择$d_0$为一个给定的行为策略的分布，该行为策略可以与目标策略不同。 为了计算$J(\theta)$的梯度，我们需要首先计算对任意状态$s \in \mathcal{S}$的状态值$v_\mu(s)$的梯度。 
**引理10.1（$v_\mu(s)$的梯度）**。当$\gamma \in (0,1)$，对于任意$s \in \mathcal{S}$有 $$ \nabla_\theta v_\mu(s) = \sum_{s' \in \mathcal{S}} \text{Pr}_\mu(s'|s) \nabla_\theta \mu(s') \left.(\nabla_a q_\mu(s',a))\right|_{a=\mu(s')}, \tag{10.16} $$ 其中 $$ \text{Pr}_\mu(s'|s) \doteq \sum_{k=0}^\infty \gamma^k [P_\mu^k]_{ss'} = \left[(I - \gamma P_\mu)^{-1}\right]_{ss'} $$ 是在策略$\mu$下从状态$s$转移到状态$s'$的折扣总概率。这里$[\cdot]_{ss'}$代表矩阵中$s$行$s'$列的元素。
**定理10.3（有折扣的情况下的确定性策略梯度定理）**。在折扣因子$\gamma \in (0,1)$的情况下，式(10.15)中给出的$J(\theta)$的梯度是 $$ \begin{align*} \nabla_\theta J(\theta) &= \sum_{s\in\mathcal{S}} \rho_\mu(s) \nabla_\theta \mu(s) \left.(\nabla_a q_\mu(s,a))\right|_{a=\mu(s)} \\ &= \mathbb{E}_{S\sim\rho_\mu} \left[ \nabla_\theta \mu(S) \left.(\nabla_a q_\mu(S,a))\right|_{a=\mu(S)} \right], \end{align*} $$ 其中状态分布$\rho_\mu$是 $$ \rho_\mu(s) = \sum_{s'\in\mathcal{S}} d_0(s') \text{Pr}_\mu(s|s'), \quad s \in \mathcal{S}. $$ 这里$\text{Pr}_\mu(s|s') = \sum_{k=0}^\infty \gamma^k [P_\mu^k]_{s's} = \left[(I - \gamma P_\mu)^{-1}\right]_{s's}$是在策略$\mu$下从$s'$转移到$s$的折扣总概率。
##### 目标函数2：平均状态值
平均奖励值的定义是 $$ \begin{align*} J(\theta) &= \bar{r}_\mu = \sum_{s\in\mathcal{S}} d_\mu(s) r_\mu(s) \\ &= \mathbb{E}_{S\sim d_\mu}[r_\mu(S)], \tag{10.20} \end{align*} $$ 其中 $$ r_\mu(s) = \mathbb{E}[R|s, a = \mu(s)] = \sum_{r} r p(r|s, a = \mu(s)) $$ 是即时奖励的期望值。
**定理10.4 (无折扣的情况下的确定性策略梯度定理)**。在无折扣的情况下，式(10.20)所示的$J(\theta)$的梯度为 $$ \begin{align*} \nabla_\theta J(\theta) &= \sum_{s\in\mathcal{S}} d_\mu(s) \nabla_\theta \mu(s) \left.(\nabla_a q_\mu(s,a))\right|_{a=\mu(s)} \\ &= \mathbb{E}_{S\sim d_\mu} \left[ \nabla_\theta \mu(S) \left.(\nabla_a q_\mu(S,a))\right|_{a=\mu(S)} \right], \end{align*} $$ 其中$d_\mu$是在策略$\mu$下状态的平稳分布。
###### 伪代码
***
初始化：给定的行为策略$\beta(a|s)$。确定性目标策略$\mu(s,\theta_0)$，其中$\theta_0$是初始参数。价值函数$q(s,a,w_0)$，其中$w_0$是初始参数。$\alpha_w,\alpha_\theta > 0$。 
目标：学习一个最优策略以最大化$J(\theta)$。 
- 在每个回合的$t$时刻 
	- 根据$\beta$生成$a_t$，然后观察$r_{t+1},s_{t+1}$ - 时序差分误差： $$ \delta_t = r_{t+1} + \gamma q(s_{t+1}, \mu(s_{t+1},\theta_t), w_t) - q(s_t, a_t, w_t) $$ 
	- Actor（策略更新）： $$ \theta_{t+1} = \theta_t + \alpha_\theta \nabla_\theta \mu(s_t,\theta_t) \left.(\nabla_a q(s_t,a,w_t))\right|_{a=\mu(s_t)} $$ 
	- Critic（价值更新）： $$ w_{t+1} = w_t + \alpha_w \delta_t \nabla_w q(s_t, a_t, w_t) $$
***
