---
date: 2025-10-31
---
#### 增量式和非增量式
- 增量式（Incremental） 指**逐步、分阶段**处理任务的方式：每次只处理一部分内容，基于上一步的结果进行迭代优化，最终累积得到完整结果。
- 非增量式（Non-incremental） 指**一次性、整体**处理任务的方式：在开始前明确完整目标，通过单次计算或操作直接得到最终结果。 
#### TD算法
给定一个策略$\pi$，我们的目标是估计所有$s \in S$的状态值$v_\pi(s)$。假设我们有一些由$\pi$生成的经验样本$(s_0, r_1, s_1, \dots, s_t, r_{t+1}, s_{t+1}, \dots)$，其中$t = 0,1,2,\dots$表示采样时刻。下面的TD算法可以使用这些样本来估计状态值： $$v_{t+1}(s_t) = v_t(s_t) - \alpha_t(s_t)\left[v_t(s_t) - \left(r_{t+1} + \gamma v_t(s_{t+1})\right)\right], \tag{7.1}$$$$v_{t+1}(s) = v_t(s),\quad \text{当} \ s \neq s_t, \tag{7.2}$$
其中$v_t(s_t)$是在$t$时刻对$v_\pi(s_t)$的估计，$\alpha_t(s_t)$是在$t$时刻对于状态$s_t$的学习率（learning rate）。
在$t$时刻，只有当时正在被访问的状态$s_t$的估计值会被更新；而所有其他未被访问的状态的估计值保持不变。
##### 性质分析
1. TD算法中每一项的含义： $$\underbrace{v_{t+1}(s_t)}_{\text{新的估计值}} = \underbrace{v_t(s_t)}_{\text{当前估计值}} - \alpha_t(s_t) \overbrace{\left[v_t(s_t) - \left(r_{t+1} + \gamma v_t(s_{t+1})\right)\right]}^{\text{TD误差}},\tag{7.6}$$ 其中 $$r_{t+1} + \gamma v_t(s_{t+1}) \doteq \bar{v}_t$$被称为**TD目标**（TD target）：$$|v_{t+1}(s_t) - \bar{v}_t| < |v_t(s_t) - \bar{v}_t|.$$新的值$v_{t+1}(s_t)$比旧的值$v_t(s_t)$更接近$\bar{v}_t$$$v_t(s_t) - \left(r_{t+1} + \gamma v_t(s_{t+1})\right) = v(s_t) - \bar{v}_t \doteq \delta_t$$被称为**TD误差**（TD error）：$\delta_t = v_t(s_t) - (r_{t+1} + \gamma v_t(s_{t+1}))$反映了时刻$t$和$t+1$之间的差异。TD误差被称为“误差”的原因是它不仅反映了两个时刻之间的差异，更重要的是反映了估计值$v_t$与真实状态值$v_\pi$之间的差异。如果估计值是准确的，那么TD误差在期望意义上应该等于0。当$v_t = v_\pi$时，TD误差的期望值为 $$ \begin{align*} \mathbb{E}[\delta_t | S_t = s_t] &= \mathbb{E}\left[v_\pi(S_t) - (R_{t+1} + \gamma v_\pi(S_{t+1})) | S_t = s_t\right] \\ &= v_\pi(s_t) - \mathbb{E}\left[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s_t\right] \\ &= 0. \quad \text{（由于式(7.3)）} \end{align*} $$ 从另一个角度来说，TD误差可以被理解为**新息**（innovation），即代表从经验样本$(s_t, r_{t+1}, s_{t+1})$中得到的新的信息，这个新的信息可以用来纠正当前估计值，从而使其更准确。
2. TD算法只能估计某一给定策略的状态值，而不能直接用于寻找最优策略。
3. TD算法和MC算法都是无模型的，区别

| TD方法                                                                                                        | MC方法                                                                                                                                                                                                                                             |
| ----------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 增量式：它可以在得到一个经验样本后立即更新估计值。                                                                                   | 非增量式：它必须等到一个回合（episode）结束之后，才能用所有经验样本来更新估计值，这是因为它需要计算从某一状态到回合最后的折扣回报。                                                                                                                                                                            |
| 持续任务：由于TD算法是增量式的，因此它可以处理回合制（episodic）和持续性（continuing）的任务。                                                   | 回合制任务：由于MC算法是非增量式的，因此它只能处理回合制任务，这些任务会在有限步后结束。                                                                                                                                                                                                    |
| 自举：TD算法依赖于自举（bootstrapping），因为状态值/动作值的更新依赖于其先前估计值。因此，TD算法需要初始值。                                             | 非自举：MC算法不是自举的，因为它可以直接估计状态值/动作值，而无需初始值。                                                                                                                                                                                                           |
| 低估计方差：TD算法的估计方差较低，这是因为它涉及的随机变量较少。例如，要估计动作值$q_\pi(s_t, a_t)$，Sarsa只需要三个随机变量$R_{t+1}$、$S_{t+1}$、$A_{t+1}$的样本。 | 高估计方差：MC算法的估计方差较高，这是因为它涉及许多随机变量。例如，要估计动作值$q_\pi(s_t, a_t)$，MC算法需要$R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots$的样本。假设每个回合的步数为$L$，并且每个状态的动作数等于$\|\mathcal{A}\|$。那么，一个随机性的软策略可能有$\|\mathcal{A}\|^L$种可能的轨迹。如果我们只用少数几个回合来估计，那么估计方差较高也就不足为奇了。  |

##### TD算法的收敛性
**定理7.1 (TD算法的收敛性)**。给定一个策略$\pi$，基于式(7.1)中的TD算法，如果对所有$s \in S$都有$\sum_t \alpha_t(s) = \infty$和$\sum_t \alpha_t^2(s) < \infty$，则$v_t(s)$随着$t \to \infty$几乎必然收敛到$v_\pi(s)$。 
1. 条件$\sum_t \alpha_t(s) = \infty$和$\sum_t \alpha_t^2(s) < \infty$应该对所有$s \in S$都成立。值得注意的是，在$t$时刻，如果状态$s$被访问，则$\alpha_t(s) > 0$；否则，$\alpha_t(s) = 0$。因此，条件$\sum_t \alpha_t(s) = \infty$在理论上要求状态$s$被访问**无限次**（实际中访问足够多次即可）。所以该条件实际上是要求有**足够多的经验数据**。
2. 学习率$\alpha_t$在实际中常常被选择为一个**小的正数**。此时，条件$\sum_t \alpha_t(s) = \infty$仍然成立，但是条件$\sum_t \alpha_t^2(s) < \infty$不再成立。这样选择$\alpha_t$的原因是它能够很好地利用后面（$t$比较大时）得到的数据。否则，如果$\alpha_t$逐渐收敛到0，那么当$t$较大时得到的数据对估计的影响已经微乎其微了。当$\alpha_t$恒等于一个正数时，算法仍然可以在某种意义上**收敛**。实际中，我们之所以希望$t$比较大时数据仍然有效，其本质原因是这样可以应对时变系统。
#### 动作值估计：Sarsa（TD)
给定一个策略$\pi$，我们的目标是估计其动作值。如果有一些由$\pi$生成的经验样本：$(s_0, a_0, r_1, s_1, a_1, \dots, s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}, \dots)$，那么可以使用下面的Sarsa算法来估计动作值： $$q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[q_t(s_t, a_t) - \left(r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})\right)\right], \tag{7.12}$$ $$q_{t+1}(s, a) = q_t(s, a),\quad \text{当}(s, a) \neq (s_t, a_t),$$ 其中$q_t(s_t, a_t)$是$q_\pi(s_t, a_t)$的估计值，$\alpha_t(s_t, a_t)$是学习率。在$t$时刻，只有$(s_t, a_t)$的动作值被更新，而其他的动作值保持不变。
Sarsa是一个用于求解如下所示的贝尔曼方程的随机近似算法： $$q_\pi(s, a) = \mathbb{E}\left[R + \gamma q_\pi(S', A') \big| s, a\right],\quad \text{对任意}(s, a). \tag{7.13}$$ 方程基于动作值的(7.13)是一个贝尔曼方程。
##### Sarsa的收敛性
**定理7.2 (Sarsa的收敛性)**。给定一个策略$\pi$，基于式(7.12)中的Sarsa算法，如果$\sum_t \alpha_t(s, a) = \infty$且$\sum_t \alpha_t^2(s, a) < \infty$对于所有的$(s, a)$都成立，那么$q_t(s, a)$随着$t \to \infty$会几乎必然收敛到$q_\pi(s, a)$。
##### 学习最优策略
初始化：对于所有$(s,a)$和所有$t$，选取$\alpha_t(s,a) = \alpha > 0$。$\epsilon \in (0,1)$。所有$(s,a)$的初始值$q_0(s,a)$。从$q_0$导出的初始$\epsilon$-Greedy策略$\pi_0$。 
目标：学习最优策略从而使智能体能从给定状态$s_0$出发到达目标状态。
###### 伪代码
***
- 对于每个回合
	- 在$s_0$，根据$\pi_0(s_0)$，得到$a_0$ 
	- 在时刻$t$，如果$s_t$不是目标状态 
		- 收集经验样本$(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$：在$s_t$，执行$a_t$，通过与环境交互生成$r_{t+1}, s_{t+1}$，再根据$\pi_t(s_{t+1})$生成$a_{t+1}$ 
		- 更新$(s_t, a_t)$的值：$$q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[q_t(s_t, a_t) - \left(r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})\right)\right]$$
		- 更新$s_t$的策略： $$\pi_{t+1}(a|s_t) = 1 - \frac{\epsilon}{|\mathcal{A}(s_t)|}\left(|\mathcal{A}(s_t)| - 1\right),\quad \text{如果} \ a = \arg\max_a q_{t+1}(s_t, a)$$$$\pi_{t+1}(a|s_t) = \frac{\epsilon}{|\mathcal{A}(s_t)|},\quad \text{如果} \ a \neq \arg\max_a q_{t+1}(s_t, a)$$$s_t \leftarrow s_{t+1}$，$a_t \leftarrow a_{t+1}$
***
##### Expected Sarsa
给定一个策略$\pi$，估计该策略的动作值： $$q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[q_t(s_t, a_t) - \left(r_{t+1} + \gamma \mathbb{E}\left[q_t(s_{t+1}, A)\right]\right)\right],$$ $$q_{t+1}(s, a) = q_t(s, a),\quad \text{当}(s, a) \neq (s_t, a_t).$$ 上式中 $$\mathbb{E}\left[q_t(s_{t+1}, A)\right] = \sum_a \pi_t(a|s_{t+1}) q_t(s_{t+1}, a) \doteq v_t(s_{t+1})$$ 是在策略$\pi_t$下$q_t(s_{t+1}, a)$的期望值。
Expected Sarsa中的TD目标是$r_{t+1} + \gamma \mathbb{E}\left[q_t(s_{t+1}, A)\right]$，而Sarsa的TD目标是$r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})$。引入期望值会略微增加计算复杂度，不过它对减少估计方差是有益的，这是因为它将Sarsa涉及的随机变量$\{s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\}$减少到了$\{s_t, a_t, r_{t+1}, s_{t+1}\}$。 
Expected Sarsa算法可以被看作求解下面方程的随机近似算法： $$q_\pi(s, a) = \mathbb{E}\left[R_{t+1} + \gamma \mathbb{E}\left[q_\pi(S_{t+1}, A_{t+1}) \big| S_{t+1}\right] \big| S_t = s, A_t = a\right]. \tag{7.15}$$ 该方程实际上是贝尔曼方程的另一种表达形式。
#### 动作值估计：$n$-Step Sarsa
$n$-Step Sarsa是Sarsa的一种推广。Sarsa和蒙特卡罗算法是$n$-Step Sarsa的两种极端情况。 
动作值的定义： $$q_\pi(s, a) = \mathbb{E}\left[G_t \big| S_t = s, A_t = a\right], \tag{7.16}$$ 其中$G_t$是折扣回报： $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots.$$ $G_t$可以被写成不同的表达式： $$\text{Sarsa} \longleftarrow\quad G_t^{(1)} = R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1}),$$ $$G_t^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 q_\pi(S_{t+2}, A_{t+2}),$$ $$\vdots$$ $$n\text{-step Sarsa} \longleftarrow\quad G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^n q_\pi(S_{t+n}, A_{t+n}),$$
$$\vdots$$ $$\text{蒙特卡罗} \longleftarrow\quad G_t^{(\infty)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \dots$$ 上式中$G_t^{(1)}, G_t^{(2)}, \dots, G_t^{(n)}$的上标仅表示$G_t$的不同分解方式，它们本质上是相等的：$G_t = G_t^{(1)} = G_t^{(2)} = G_t^{(n)} = G_t^{(\infty)}$。
- 当$n=1$时，我们有 $$q_\pi(s, a) = \mathbb{E}\left[G_t^{(1)} \big| s, a\right] = \mathbb{E}\left[R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1}) \big| s, a\right].$$ 求解这个方程的随机近似算法是 $$q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[q_t(s_t, a_t) - \left(r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})\right)\right].$$ 上式就是(7.12)中的Sarsa算法。 
- 当$n=\infty$时，我们有 $$q_\pi(s, a) = \mathbb{E}\left[G_t^{(\infty)} \big| s, a\right] = \mathbb{E}\left[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \big| s, a\right].$$ 求解这个方程的随机近似算法是 $$q_{t+1}(s_t, a_t) = g_t \doteq r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots,$$ 其中$g_t$是$G_t$的一个样本。上式实际上就是蒙特卡罗方法，它使用从$(s_t, a_t)$开始的回报来近似$(s_t, a_t)$的动作值。
- 当$n$取一般的自然数时，我们有 $$q_\pi(s, a) = \mathbb{E}\left[G_t^{(n)} \big| s, a\right] = \mathbb{E}\left[R_{t+1} + \gamma R_{t+2} + \dots + \gamma^n q_\pi(S_{t+n}, A_{t+n}) \big| s, a\right].$$ 求解这个方程的随机近似算法是 $$ \begin{align*} q_{t+1}(s_t, a_t) &= q_t(s_t, a_t) \\ &\quad - \alpha_t(s_t, a_t)\left[q_t(s_t, a_t) - \left(r_{t+1} + \gamma r_{t+2} + \dots + \gamma^n q_t(s_{t+n}, a_{t+n})\right)\right]. \tag{7.17} \end{align*} $$ 这个算法被称为$n$-step Sarsa。
由于$n$-Step Sarsa包含Sarsa和蒙特卡罗这两个极端情况，因此其性能也介于Sarsa和蒙特卡罗之间。如果$n$较大，$n$-Step Sarsa接近于蒙特卡罗：其估计具有较小的偏差（bias）但较大的方差。如果$n$较小，$n$-Step Sarsa接近于Sarsa：其估计具有较小的方差但较大的偏差。 
$n$-Step Sarsa仅可用于评价一个给定的策略。
在实现$n$-Step Sarsa算法时，我们需要经验样本$(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}, \dots, r_{t+n}, s_{t+n}, a_{t+n})$。由于我们在$t$时刻还无法拿到样本$(r_{t+n}, s_{t+n}, a_{t+n})$，因此必须等到$t + n$时刻才能更新$(s_t, a_t)$的$q$值。为此，式(7.17)可以被重新写为 $$ \begin{align*} q_{t+n}(s_t, a_t) &= q_{t+n-1}(s_t, a_t) \\ &\quad - \alpha_{t+n-1}(s_t, a_t)\left[q_{t+n-1}(s_t, a_t) \right. \\ &\quad \left. - \left(r_{t+1} + \gamma r_{t+2} + \dots + \gamma^n q_{t+n-1}(s_{t+n}, a_{t+n})\right)\right], \end{align*} $$ 其中$q_{t+n}(s_t, a_t)$是在$t + n$时刻对$q_\pi(s_t, a_t)$的估计。
#### 最优动作值估计：Q-learning
Q-learning算法如下所示： $$q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[q_t(s_t, a_t) - \left(r_{t+1} + \gamma \max_{a \in \mathcal{A}} q_t(s_{t+1}, a)\right)\right], \tag{7.18}$$ $$q_{t+1}(s, a) = q_t(s, a),\quad \text{当}(s, a) \neq (s_t, a_t),$$ 其中$t = 0,1,2,\dots$。这里$q_t(s_t, a_t)$是对$(s_t, a_t)$的最优动作值的估计，而$\alpha_t(s_t, a_t)$是学习率。
##### Q-learning的表达式与Sarsa区别
TD目标：Q-learning的TD目标是$r_{t+1} + \gamma \max_{a} q_t(s_{t+1}, a)$，而Sarsa的TD目标则是$r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})$。因此，如果当前的状态-动作是$(s_t, a_t)$，Sarsa算法的更新需要样本$(r_{t+1}, s_{t+1}, a_{t+1})$，而Q-learning只需要$(r_{t+1}, s_{t+1})$。 
Q-learning是一个求解如下贝尔曼最优方程的随机近似算法： $$q(s, a) = \mathbb{E}\left[R_{t+1} + \gamma \max_a q(S_{t+1}, a) \big| S_t = s, A_t = a\right]. \tag{7.19}$$ 上面这个方程是基于动作值的贝尔曼最优方程。Q-learning 的收敛性分析与定理 7.1 类似
#### Off-policy（异策略）和On-policy（同策略）
任何一个强化学习算法都会涉及两种策略：一种是**行为策略（behavior policy）**，另一种是**目标策略（target policy）**。行为策略用于生成经验样本，而目标策略不断更新，从而收敛至最优策略。
当行为策略与目标策略相同时，该算法被称为On-policy的，中文为**同策略**（因为两个策略相同）；
当它们不同时，该算法被称为Off-policy的，中文为**异策略**（因为两个策略不同）。 Off-policy算法的优势在于它可以使用由其他策略生成的经验样本来学习最优策略。
Online/Offline（在线/离线）**在线学习**是指智能体在与环境交互的同时用生成的数据来更新值和策略。**离线学习**是指智能体不与环境交互，而是使用预先收集的数据来更新值和策略。如果算法是On-policy的，那么它可以实现在线学习，但不能实现离线学习，因为它无法使用预先收集的其他策略生成的数据。如果算法是Off-policy的，那么它既可以在线学习，也可以离线学习。
#### Q-learning 
On-policy模式
###### 伪代码
***
初始化：对所有$(s,a)$和所有$t$，$\alpha_t(s,a) = \alpha > 0$。$\epsilon \in (0,1)$。所有$(s,a)$的初始值$q_0(s,a)$。从$q_0$导出的初始$\epsilon$-Greedy策略$\pi_0$。 
目标：学习最优策略从而使智能体能从给定状态$s_0$出发到达目标状态。 
- 对于每个回合 
	- 在$t$时刻，如果$s_t$不是目标状态 
		- 收集经验样本$(a_t, r_{t+1}, s_{t+1})$：在$s_t$，根据$\pi_t(s_t)$产生$a_t$，通过与环境互动生成$r_{t+1}, s_{t+1}$。 
		- 更新$(s_t, a_t)$的值： $$q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[q_t(s_t, a_t) - \left(r_{t+1} + \gamma \max_a q_t(s_{t+1}, a)\right)\right]$$
		- 更新$s_t$的策略： $$\pi_{t+1}(a|s_t) = 1 - \frac{\epsilon}{|\mathcal{A}(s_t)|}\left(|\mathcal{A}(s_t)| - 1\right),\quad \text{如果} \ a = \arg\max_a q_{t+1}(s_t, a)$$ $$\pi_{t+1}(a|s_t) = \frac{\epsilon}{|\mathcal{A}(s_t)|},\quad \text{如果} \ a \neq \arg\max_a q_{t+1}(s_t, a)$$
***
Off-policy模式
###### 伪代码
***
初始化：所有$(s,a)$的初始值$q_0(s,a)$。所有$(s,a)$的行为策略$\pi_b(a|s)$。对所有$(s,a)$和所有$t$，$\alpha_t(s,a) = \alpha > 0$。 
目标：使用$\pi_b$生成的经验数据，学习所有状态的最优策略$\pi_T$。 
- 对$\pi_b$生成的每个回合$\{s_0, a_0, r_1, s_1, a_1, r_2, \dots\}$ 
	- 对回合中的每一步$t = 0,1,2,\dots$ - 更新$(s_t, a_t)$的值： $$q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[q_t(s_t, a_t) - \left(r_{t+1} + \gamma \max_a q_t(s_{t+1}, a)\right)\right]$$ 
	- 更新$s_t$的目标策略： $$\pi_{T,t+1}(a|s_t) = 1,\quad \text{如果} \ a = \arg\max_a q_{t+1}(s_t, a)$$ $$\pi_{T,t+1}(a|s_t) = 0,\quad \text{如果} \ a \neq \arg\max_a q_{t+1}(s_t, a)$$
#### 时序差分算法统一框架
用于动作值估计的TD算法可以写成一个统一的表达式： $$q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[q_t(s_t, a_t) - \bar{q}_t\right],$$
其中$\bar{q}_t$是TD目标。所有的TD算法都可以用以上来描述，只是不同的TD算法有不同的TD目标$\bar{q}_t$，

| 算法             | TD 目标 $\bar{q}_t$ 的表达式                                                          | 求解的数学方程                                                                                                                                | 模式         |
| -------------- | ------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- | ---------- |
| Sarsa          | $\bar{q}_t = r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})$                            | BE: $q_\pi(s, a) = \mathbb{E}\left[R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1}) \big\| S_t = s, A_t = a\right]$                            | On-policy  |
| $n$-step Sarsa | $\bar{q}_t = r_{t+1} + \gamma r_{t+2} + \dots + \gamma^n q_t(s_{t+n}, a_{t+n})$ | BE: $q_\pi(s, a) = \mathbb{E}\left[R_{t+1} + \gamma R_{t+2} + \dots + \gamma^n q_\pi(S_{t+n}, A_{t+n}) \big\| S_t = s, A_t = a\right]$ |            |
| Q-learning     | $\bar{q}_t = r_{t+1} + \gamma \max_a q_t(s_{t+1}, a)$                           | BOE: $q(s, a) = \mathbb{E}\left[R_{t+1} + \gamma \max_a q(S_{t+1}, a) \big\| S_t = s, A_t = a\right]$                                  | Off-policy |
| Monte Carlo    | $\bar{q}_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots$               | BE: $q_\pi(s, a) = \mathbb{E}\left[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \big\| S_t = s, A_t = a\right]$                 | On-policy  |
表中 BE 代表贝尔曼方程，BOE 代表贝尔曼最优方程
