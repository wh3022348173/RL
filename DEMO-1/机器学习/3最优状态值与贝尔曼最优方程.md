---
date: 2025-10-17
---
#### 定理 最优策略与最优状态值。
考虑策略 $\pi^*$，如果对任意的状态 $s \in \mathcal{S}$ 和其他任意策略 $\pi$，都有 $v_{\pi^*}(s) \geq v_\pi(s)$，那么 $\pi^*$ 是一个**最优策略**，并且 $\pi^*$ 对应的状态值是**最优状态值**。 

上述定义表明：**一个最优策略在每一个状态都有比所有其他策略更高的状态值**。

对于每个 $s\in \mathcal{S}$，贝尔曼最优方程的表达式如下所示： $$ \begin{align*} v(s) &= \max_{\pi(s) \in \Pi(s)} \sum_{a \in \mathcal{A}} \pi(a|s) \left( \sum_{r \in \mathcal{R}} p(r|s,a) r + \gamma \sum_{s' \in \mathcal{S}} p(s'|s,a) v(s') \right) \\ &= \max_{\pi(s) \in \Pi(s)} \sum_{a \in \mathcal{A}} \pi(a|s) q(s,a), \end{align*} \tag{3.1} $$ 其中 $$ q(s,a) \doteq \sum_{r \in \mathcal{R}} p(r|s,a) r + \gamma \sum_{s' \in \mathcal{S}} p(s'|s,a) v(s'). $$这里 $v(s), v(s')$ 是待求解的未知量；$\pi(s)$表示状态 $s$ 的策略；$\Pi(s)$是在状态 $s$所有可能策略的集合。
具体来说，假设有$n$个状态$\{s_1, s_2, \dots, s_n\}$。我们可以得到贝尔曼最优方程的矩阵-向量形式为

$$

v = \max_{\pi \in \Pi} (r_\pi + \gamma P_\pi v), \tag{3.2}

$$

其中$v = [v(s_1), v(s_2), \dots, v(s_n)]^\text{T} \in \mathbb{R}^n$是待求解的未知量。上式中的$\max_\pi$是以逐元素的方式执行的。此外，上式中$r_\pi$和$P_\pi$与贝尔曼方程的矩阵-向量形式中的相同：

$$

[r_\pi]_s \doteq \sum_{a \in A} \pi(a|s) \sum_{r \in \mathcal{R}} p(r|s,a) r, \quad [P_\pi]_{s,s'} = p(s'|s) \doteq \sum_{a \in A} \pi(a|s) p(s'|s,a).

$$
由于$\pi$的最优值由$v$决定，式的右侧实际上是$v$的函数，因此可以用一个函数$f(v)$表示为

$$

f(v) \doteq \max_{\pi \in \Pi} (r_\pi + \gamma P_\pi v).

$$

然后，贝尔曼最优方程(3.2)可以简洁地表达为

$$

v = f(v). \tag{3.3}

$$
考虑一个函数$f(x)$，其中$x \in \mathbb{R}^d$且$f: \mathbb{R}^d \to \mathbb{R}^d$。如果一个点$x^*$满足 $$f(x^*) = x^*,$$ 那么称之为不动点（fixed point）。之所以称之为“不动点”，是因为$x^*$的映射还是其自身。 如果存在$\gamma \in (0,1)$使得 $$\| f(x_1) - f(x_2) \| \leqslant \gamma \| x_1 - x_2 \| $$对于任意的$x_1, x_2 \in \mathbb{R}^d$都成立，那么函数$f$被称为压缩映射（contraction mapping）。上面不等式中的$\| \cdot \|$表示向量或矩阵的范数。
#### 定理  (压缩映射定理)。
对于方程$x = f(x)$，其中$x$和$f(x)$是实数向量。如果$f$是一个压缩映射，则下面所有性质都成立。

- 存在性：一定存在一个不动点$x^*$满足$f(x^*) = x^*$。

- 唯一性：不动点$x^*$是唯一的。

- 算法：考虑迭代算法

  $$

  x_{k+1} = f(x_k),

  $$

  其中$k = 0,1,2,\dots$。给定任意一个初始值$x_0$，当$k \to \infty$时，$x_k \to x^*$，且收敛过程具有指数收敛速度。

#### 定理   $f(v)$的压缩性质
贝尔曼最优方程(3.3)右侧的函数$f(v)$是一个压缩映射

即对于任意的$v_1, v_2 \in \mathbb{R}^{|S|}$，有
$$
\| f(v_1) - f(v_2) \|_\infty \leqslant \gamma \| v_1 - v_2 \|_\infty,

$$

其中$\gamma \in (0,1)$是折扣率，$\| \cdot \|_\infty$是最大值范数，即向量中所有元素的最大绝对值。

求解最优状态值$v^*$：如果$v^*$是贝尔曼最优方程的解，那么它满足
$$
v^* = f(v^*) = \max_{\pi \in \Pi} (r_\pi + \gamma P_\pi v^*).
$$
显然，$v^*$是一个不动点。根据压缩映射定理，有如下重要结论。
#### 定理  存在性、唯一性、算法
贝尔曼最优方程$v = f(v) = \max_{\pi \in \Pi} (r_\pi + \gamma P_\pi v)$始终存在唯一解$v^*$，该解可以通过如下迭代算法求解：
$$
v_{k+1} = f(v_k) = \max_{\pi \in \Pi} (r_\pi + \gamma P_\pi v_k), \quad k = 0,1,2,\dots
$$
对任意给定的$v_0$，当$k \to \infty$时，$v_k$以指数速度收敛至$v^*$。

- 存在性：贝尔曼最优方程的解总是存在的。

- 唯一性：贝尔曼最优方程的解总是唯一的。

- 算法：贝尔曼最优方程可以通过定理3.3中的迭代算法求解。此迭代算法有一个

名字——值迭代。该算法的**具体实施步骤**将在第4章详细介绍，本章主要关注

贝尔曼最优方程的基本性质。

**求解最优策略$\pi^*$**：一旦得到了$v^*$的值，就可以通过下式求解得到一个最优策略：
$$
\pi^* = \arg\max_{\pi \in \Pi} (r_\pi + \gamma P_\pi v^*). \tag{3.6}
$$
$\pi^*$的具体形式将在定理3.5中给出。现在将式(3.6)代入贝尔曼最优方程中可得
$$
v^* = r_{\pi^*} + \gamma P_{\pi^*} v^*.
$$
因此，$v^* = v_{\pi^*}$是策略$\pi^*$的状态值。从上式可以看出，贝尔曼最优方程是一个特殊的贝尔曼方程，其对应的策略是$\pi^*$。

#### 定理 3.4   $v^*$和$\pi^*$的最优性
如果$v^*$和$\pi^*$是贝尔曼最优方程的解，那么$v^*$是最优状态值，而$\pi^*$是最优策略，即对于任意策略$\pi$都有
$$
v^* = v_{\pi^*} \geqslant v_\pi,
$$
其中$v_\pi$是策略$\pi$的状态值，“$\geqslant$”是逐元素比较。

#### 定理  贪婪最优策略
假设$v^*$是贝尔曼最优方程的最优状态值解，那么下面的确定性贪婪策略是一个最优策略：
$$
\pi^*(a|s) = \begin{cases}

1, & a = a^*(s), \\

0, & a \neq a^*(s),

\end{cases} \quad s \in S, \tag{3.7}
$$
其中
$$
a^*(s) = \arg\max_a q^*(a,s),
$$

且
$$
q^*(s,a) \doteq \sum_{r \in \mathcal{R}} p(r|s,a) r + \gamma \sum_{s' \in S} p(s'|s,a) v^*(s').
$$
#### 影响最优策略的因素
$$
v(s) = \max_{\pi(s) \in \Pi(s)} \sum_{a \in \mathcal{A}} \pi(a|s) \left( \sum_{r \in \mathcal{R}} p(r|s,a) r + \gamma \sum_{s' \in S} p(s'|s,a) v(s') \right), \quad s \in S.
$$

这里的未知量是$v^*$和$\pi^*$，已知量包括即时奖励$r$、折扣因子$\gamma$、系统模型$p(s'|s,a), p(r|s,a)$。显然，这里的未知量（即最优策略和最优状态值）是由这些已知量决定的。如果系统模型是给定的，那么最优策略会受到$r$和$\gamma$的影响，下面我们讨论当改变$r$和$\gamma$时
#### 定理  最优策略的不变性
考虑一个马尔可夫决策过程，假设$v^* \in \mathbb{R}^{|S|}$为最优状态值，即满足$v^* = \max_{\pi \in \Pi} (r_\pi + \gamma P_\pi v^*)$。如果每个奖励$r$都通过仿射变换变成$\alpha r + \beta$，其中$\alpha, \beta \in \mathbb{R}$且$\alpha > 0$，那么相应的最优状态值$v'$也是$v^*$的一个仿射变换：
$$
v' = \alpha v^* + \frac{\beta}{1 - \gamma} \mathbf{1}, \tag{3.8}
$$

这里$\gamma \in (0,1)$是折扣因子；$\mathbf{1} = [1, \dots, 1]^\text{T}$。$v'$对应的最优策略与$v^*$对应的最优策略相同。

最优策略对奖励的仿射变换是保持不变的。换句话说，如果我们对所有奖励进行同比例缩放或加
减相同的值，那么最优策略仍然保持不变。

最优状态值是最优策略的状态值；最优策略是可以基于最优状态值得到的。