---
date: 2025-10-25
---
#### 值迭代
迭代算法的每次迭代包含两个步骤。 
- 第一步是**策略更新（policy update）**。在数学上，它旨在找到一个能解决以下优化问题的策略： $$\pi_{k+1} = \arg\max_{\pi}(r_\pi + \gamma P_\pi v_k),$$ 其中$v_k$是上一次迭代得到的值。 按元素展开形式为 $$\pi_{k+1}(s) = \arg\max_\pi \sum_a \pi(a|s) \underbrace{\left( \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_k(s') \right)}_{q_k(s,a)}, \quad s \in \mathcal{S}.$$ 上述优化问题的最优解为 $$\pi_{k+1}(a|s) = \begin{cases} 1, & a = a_k^*(s), \\ 0, & a \neq a_k^*(s), \end{cases} \tag{4.2}$$ 其中$a_k^*(s) = \arg\max_a q_k(s,a)$。此外，如果有多个动作的值都相同并且最大，那么可以选择其中任意一个动作。
- 第二步是**值更新（value update）**。在数学上，它通过下式来计算新的值$v_{k+1}$： $$v_{k+1} = r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_k, \tag{4.1}$$ 其中$v_{k+1}$将用于下一次迭代。按元素展开形式是 $$v_{k+1}(s) = \sum_a \pi_{k+1}(a|s) \underbrace{\left( \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_k(s') \right)}_{q_k(s,a)}, \quad s \in \mathcal{S}.$$ 将式(4.2)代入上式可得 $$v_{k+1}(s) = \max_a q_k(s,a)$$
  k有限时，$v_k$不是状态值，因为不满足任何一个贝尔曼方程，k趋于无穷才会收敛到最优状态值
  即新的值等于状态对应的最大值。 上面两个步骤可以概述为如下形式： $$v_k(s) \to \text{计算} q_k(s,a) \to \text{计算新策略} \pi_{k+1}(s) \to \text{计算新值} v_{k+1}(s) = \max_a q_k(s,a)$$
###### 伪代码
***
初始化：已知模型，即任意$(s,a)$对应的$p(r|s,a)$和$p(s'|s,a)$。
初始值$v_0$。 目标：求解最优状态值和最优策略。 
当$v_k$尚未收敛时（例如$\|v_k - v_{k-1}\|$大于给定阈值时），进行如下迭代 
- 对每个状态 $s \in \mathcal{S}$ 
	- 对每个动作 $a \in \mathcal{A}(s)$ 计算$q$值：$q_k(s,a) = \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_k(s')$ 
		- 最大价值动作：$a_k^*(s) = \arg\max_a q_k(s,a)$ 
		- 策略更新：$\pi_{k+1}(a|s) = 1$ 如果 $a = a_k^*$；否则 $\pi_{k+1}(a|s) = 0$ 
		- 值更新：$v_{k+1}(s) = \max_a q_k(s,a)$
***
#### 策略迭代算法
- 第一步是**策略评价（policy evaluation）**。评估上一次迭代得到的策略$\pi_k$。从数学上来说，该步骤就是求解下面的贝尔曼方程，从而得到$\pi_k$的状态值： $$v_{\pi_k} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}, \tag{4.3}$$ 其中$\pi_k$是上一次迭代中得到的策略，$v_{\pi_k}$是该策略对应的状态值，$r_{\pi_k}$和$P_{\pi_k}$可以根据系统模型得到。 
  元素展开形式为 $$v_{\pi_k}^{(j+1)}(s) = \sum_a \pi_k(a|s) \left( \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_{\pi_k}^{(j)}(s') \right), \quad s \in \mathcal{S},$$ 其中。 
- 第二步是**策略改进（policy improvement）**。改进策略从而得到更好的新策略。从数学上来说，在第一步得到$v_{\pi_k}$之后，此步骤会利用下式得到新的策略$\pi_{k+1}$： $$\pi_{k+1} = \arg\max_{\pi}(r_\pi + \gamma P_\pi v_{\pi_k}).$$
  元素展开形式为  $$\pi_{k+1}(s) = \arg\max_\pi \sum_a \pi(a|s) \underbrace{\left( \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_{\pi_k}(s') \right)}_{q_{\pi_k}(s,a)}, \quad s \in S.$$
  其中是策略对应的动作值。令为最大值动作，那么上述优化问题的解是 $$\pi_{k+1}(a|s) = \begin{cases} 1, & a = a_k^*(s), \\ 0, & a \neq a_k^*(s). \end{cases}$$
计算状态值
- 解析法
- 数值迭代法（use）
策略迭代本身是一个迭代算法，而每次迭代中策略评价步骤需要调用另一个迭代算法
##### $\pi_{k+1}$优于$\pi_k$

**引理4.1 (策略改进)**。如果$\pi_{k+1} = \arg\max_{\pi}(r_\pi + \gamma P_\pi v_{\pi_k})$，那么$v_{\pi_{k+1}} \geq v_{\pi_k}$。 上述引理中，“$\geq$”是逐元素的比较，即$v_{\pi_{k+1}} \geq v_{\pi_k}$意味着对任意状态$s$有$v_{\pi_{k+1}}(s) \geq v_{\pi_k}(s)$。因此，$\pi_{k+1}$优于$\pi_k$。
##### 策略迭代算法收敛到最优策略

**定理4.1 (策略迭代的收敛性)**。由策略迭代算法生成的状态值序列$\{v_{\pi_k}\}_{k=0}^\infty$收敛于最优状态值$v^*$。因此，策略序列$\{\pi_k\}_{k=0}^\infty$收敛于一个最优策略。
###### 伪代码
***
初始化：已知模型，即任意$(s,a)$对应的$p(r|s,a)$和$p(s'|s,a)$，初始策略$\pi_0$。 
目标：寻找最优状态值和最优策略。 
- 当$v_{\pi_k}$未收敛时，进行第$k$次迭代 
	- 策略评价： 
		- 选择初始值$v_{\pi_k}^{(0)}$ 
		- 当$v_{\pi_k}^{(j)}$未收敛时，进行第$j$次迭代
			- 对每一个状态$s \in \mathcal{S}$ $$v_{\pi_k}^{(j+1)}(s) = \sum_a \pi_k(a|s) \left[ \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_{\pi_k}^{(j)}(s') \right]$$	 
	- 策略改进： 
		- 对每一个状态$s \in \mathcal{S}$ 
			- 对每一个动作$a \in \mathcal{A}$ $$q_{\pi_k}(s,a) = \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_{\pi_k}(s')$$$a_k^*(s) = \arg\max_a q_{\pi_k}(s,a)$ 
			 $\pi_{k+1}(a|s) = 1$如果$a = a_k^*$；否则$\pi_{k+1}(a|s) = 0$
**规律**
1. 接近目标区域的状态能更早地找到最优策略
2. 接近目标区域的状态的状态值更高
#### 截断策略迭代
对比值迭代和策略迭代

|          | 策略迭代算法                                                                         | 值迭代算法                                              | 补充说明                                                                                                                                 |
| -------- | ------------------------------------------------------------------------------ | -------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| 策略       | $\pi_0$                                                                        | 无                                                  |                                                                                                                                      |
| 值        | $v_{\pi_0} = r_{\pi_0} + \gamma P_{\pi_0} v_{\pi_0}$ \| $v_0 \doteq v_{\pi_0}$ | $v_0 \doteq v_{\pi_0}$                             |                                                                                                                                      |
| 策略       | $\pi_1 = \arg\max_\pi (r_\pi + \gamma P_\pi v_{\pi_0})$                        | $\pi_1 = \arg\max_\pi (r_\pi + \gamma P_\pi v_0)$  | 这两个策略是相同的                                                                                                                            |
| 值        | $v_{\pi_1} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}$                           | $v_1 = r_{\pi_1} + \gamma P_{\pi_1} v_0$           | $v_{\pi_1} \geqslant v_{\pi_0}$ 因为 $v_{\pi_1} \geqslant v_{\pi_0}$$v_{\pi_1} \geqslant v_{\pi_0}$ 因为 $v_{\pi_1} \geqslant v_{\pi_0}$ |
| 策略       | $\pi_2 = \arg\max_\pi (r_\pi + \gamma P_\pi v_{\pi_1})$                        | $\pi_2' = \arg\max_\pi (r_\pi + \gamma P_\pi v_1)$ |                                                                                                                                      |
| $\vdots$ | $\vdots$                                                                       | $\vdots$                                           | $\vdots$                                                                                                                             |

$$v_{\pi_1}^{(0)} = v_0$$ 值迭代算法 $\leftarrow v_1 \leftarrow v_{\pi_1}^{(1)} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}^{(0)}$ $$v_{\pi_1}^{(2)} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}^{(1)}$$ $$\vdots$$ 截断策略迭代算法 $\leftarrow \bar{v}_1 \leftarrow v_{\pi_1}^{(j)} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}^{(j-1)}$ $$\vdots$$ 策略迭代算法 $\leftarrow v_{\pi_1} \leftarrow v_{\pi_1}^{(\infty)} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}^{(\infty)}$ 

**截断策略迭代**是介于值迭代和策略迭代之间的算法。一方面，它比值迭代算法收敛得更快，因为它在策略评价中计算了多次迭代（而非一次）；另一方面，它比策略迭代算法收敛得更慢，因为它只计算了有限次迭代（而非无穷次）。这种直观与下面的数学分析是一致的。
![[4-1.png|500]]
###### 伪代码
***
初始化：已知模型，即任意$(s,a)$对应的$p(r|s,a)$和$p(s'|s,a)$。初始策略$\pi_0$。 
目标：寻找最优状态值和最优策略。 
- 当$v_k$未收敛时，进行第$k$次迭代 
	- 策略评价： 
	- 选择初始值为$v_k^{(0)} = v_{k-1}$。设置最大迭代次数为$j_{\text{truncate}}$。 
	- 当$j < j_{\text{truncate}}$时 
		- 对每一个状态$s \in S$
			- $v_k^{(j+1)}(s) = \sum_a \pi_k(a|s) \left[ \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_k^{(j)}(s') \right]$ 
	- 令$v_k = v_k^{(j_{\text{truncate}})}$ 
	- 策略改进： 
	- 对每一个状态$s \in S$ 
		- 对每一个动作$a \in \mathcal{A}(s)$ 
			-  $q_k(s,a) = \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_k(s')$ 
		 - $a_k^*(s) = \arg\max_a q_k(s,a)$ 
		 - $\pi_{k+1}(a|s) = 1$如果$a = a_k^*$；否则$\pi_{k+1}(a|s) = 0$
***
**优势**：与策略迭代算法相比，它在策略评价步骤中仅需要有限次迭代（而非无限次），因此计算效率更高。与值迭代算法相比，它在策略评价步骤中运行了多次迭代（而非一次），因此可以得到更好的估计值。

