---
date: 2025-10-28
---
策略迭代算法的每次迭代有两个步骤。
1. 策略评估，旨在通过求解贝尔曼方程$v_{\pi_k} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}$以得到$v_{\pi_k}$；
2. 策略改进，旨在计算贪婪策略$\pi_{k+1} = \arg\max_\pi (r_\pi + \gamma P_\pi v_{\pi_k})$以得到一个更好的策略。具体来说，策略改进步骤的元素展开形式是

$$
\begin{align*}

\pi_{k+1}(s) &= \arg\max_\pi \sum_a \pi(a|s) \left[ \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a) v_{\pi_k}(s') \right] \\

&= \arg\max_\pi \sum_a \pi(a|s) q_{\pi_k}(s,a),\quad s \in \mathcal{S}.

\end{align*}
$$
从上式能看出，**动作值**$q_{\pi_k}(s,a)$是策略迭代算法的核心：
1. 策略评估就是在**计算状态值进而得到动作值**；
2. 策略改进就是**选取动作值最大的动作**作为新策略。
#### 计算动作值的方法
  1. 基于模型的方法。首先求解贝尔曼方程得到状态值$v_{\pi_k}$，然后基于下式得到动作值： $$ q_{\pi_k}(s,a) = \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_{\pi_k}(s'). \tag{5.1} $$ 这种方法需要知道**模型$\{p(r|s,a), p(s'|s,a)\}$**。策略迭代算法采用的就是这种方法。
  2. 无需模型的方法。动作价值的原始定义： $$\begin{align*} q_{\pi_k}(s,a) &= \mathbb{E}[G_t|S_t = s, A_t = a] \\ &= \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots | S_t = s, A_t = a], \end{align*}$$因为$q_{\pi_k}(s,a)$是一个期望值，可以通过蒙特卡罗方法用数据来估计：从$(s,a)$开始，智能体可以执行策略$\pi_k$，进而获得$n$个回合，假设第$i$个回合的回报是$g_{\pi_k}^{(i)}(s,a)$。那么，这些回合的回报的平均值可以用来近似$q_{\pi_k}(s,a)$，即 $$ q_{\pi_k}(s,a) = \mathbb{E}[G_t|S_t = s, A_t = a] \approx \frac{1}{n} \sum_{i=1}^n g_{\pi_k}^{(i)}(s,a). \tag{5.2} $$ 根据大数定律，如果$n$足够大，上面的近似将会足够精确。
 基于**蒙特卡罗**的强化学习的基本思想就是使用(5.2)来估计动作值，从而代替策略迭代算法中需要模型的模块。
#### MC Basic算法
1. 策略评估。这一步用于估算所有$(s,a)$的$q_{\pi_k}(s,a)$。具体来说，对于每个$(s,a)$，收集足够多的回合进而求其回报的平均值（记作$q_k(s,a)$）来近似$q_{\pi_k}(s,a)$。 
2. 策略改进。这一步通过$\pi_{k+1}(s) = \arg\max_\pi \sum_a \pi(a|s)q_k(s,a)$得到所有$s \in \mathcal{S}$的新策略，即$\pi_{k+1}(a_k^*|s) = 1$，其中$a_k^* = \arg\max_a q_k(s,a)$。
###### 伪代码
***
初始化: 初始策略$\pi_0$ 
目标: 寻找最优策略。 
- 对于第$k$次迭代（$k=0,1,2,\dots$） 　　
	- 对于每个状态$s \in \mathcal{S}$ 　　　　
		- 对于每个动作$a \in \mathcal{A}(s)$ 　　　　　　
			- 执行$\pi_k$收集从$(s,a)$开始的足够多的回合 　　　　　　
			- 策略评估： 　　　　　　
				-  $q_{\pi_k}(s,a) \approx q_k(s,a) =$所有从$(s,a)$开始的回合的回报的平均值 　　　　　　
			- 策略改进： 　　　　　　
				-  $a_k^*(s) = \arg\max_a q_k(s,a)$ 　　　　　　
			     - 如果$a = a_k^*$，则$\pi_{k+1}(a|s) = 1$；否则$\pi_{k+1}(a|s) = 0$
实际上无法对每一个$(s,a)$收集足够多的回合，此时动作价值的近似不准确
每个回合必须足够长，但不需要无限长
**稀疏奖励**：除非到达目标状态，负责无法获得任何正奖励（回合必须到达目标）稀疏奖励降低了学习效率（设计非稀疏奖励或稠密奖励）
#### MC Exploring Starts
##### 更高效地利用样本
   $$ s_1 \stackrel{a_2}{\to} s_2 \stackrel{a_4}{\to} s_1 \stackrel{a_2}{\to} s_2 \stackrel{a_3}{\to} s_5 \stackrel{a_1}{\to} \dots \tag{5.3} $$
   如果一个状态-动作配对在一个回合中出现了一次，那么我们称该**状态-动作**被**访问**（visit）一次。
   最简单的方法是：一个回合仅用于估计该回合最开始访问的状态-动作的价值，MC Basic的算法就是基于这种方法的，没有充分利用样本。
   **一个回合也可以被用于估计其他状态-动作的价值。**
   在一个回合中，一个状态-动作可能会被多次访问。
   1. 只考虑第一次访问，这种方法被称为**首次访问**（first visit）。（以第一次出现的$(s_1,a_2)$为开始的子回合会被用来估计$(s_1,a_2)$的动作值）。
   2. 考虑每一次访问，则这种方法被称为**每次访问**（every visit）。（每次出现$(s_1,a_2)$时，以其为开始的子回合会被用来估计其动作值）。
   在样本使用效率方面，**每次访问**都利用的方法效率是最高的。
##### 更高效得更新策略
1. 在策略评价步骤中，收集从某一个状态-动作开始的所有回合，然后使用所有回合的平均回报来近似动作值，进而再更新策略（在MC Basic算法中被采用）。
	- 缺点：智能体必须等到所有回合都被收集完毕后才能估计动作值。

2. 在策略评价步骤中，收集从某一个状态-动作开始的单个回合，然后使用这个单个回合的回报来近似动作值，进而再立即更新策略（广义策略迭代）。
	- 优点：不需要等到收集完所有回合，而是可以在得到一个回合后立即更新值和策略。
**算法条件：** 对每一个状态动作，都要有足够多的回合从它出发
###### 伪代码
***
初始化：初始策略$\pi_0(a|s)$，所有$(s,a)$的初始价值$q(s,a)$。$\text{Return}(s,a) = 0$和$\text{Number}(s,a) = 0$适用于所有$(s,a)$。 
目标：寻找一个最优策略。 
- 对每个回合 
	- 回合生成：选择一个起始状态-动作$(s_0, a_0)$（确保所有状态-动作都可能被选中，这就是Exploring Starts条件）。按照当前策略，生成一个长度为$T$的回合：$s_0, a_0, r_1, \dots, s_{T-1}, a_{T-1}, r_T$。 
	- 每个回合的初始化：$g \leftarrow 0$ 
	- 对回合中的每一步$t = T-1, T-2, \dots, 0$ 
		- $g \leftarrow \gamma g + r_{t+1}$ 
		- $\text{Return}(s_t, a_t) \leftarrow \text{Return}(s_t, a_t) + g$ 
		- $\text{Number}(s_t, a_t) \leftarrow \text{Number}(s_t, a_t) + 1$ 
		- 策略评价：
			- $q(s_t, a_t) \leftarrow \text{Return}(s_t, a_t)/\text{Number}(s_t, a_t)$ 
		- 策略改进： 
			- 对$a = \arg\max_a q(s_t, a)$，$\pi(a|s_t) = 1$；对其他$a$，$\pi(a|s_t) = 0$
***
#### MC $\epsilon$-Greedy算法 
**软策略（soft policy）**
如果一个策略能在任意状态下有非零概率选择任意动作
给定一个软策略，即使只有一个回合，只要这个回合足够长，它就会多次访问每个状态-动作。从而Exploring Starts这个条件就可以避免了。
##### $\epsilon$-Greedy策略
$\epsilon \in [0,1]$ $$ \pi(a|s) = \begin{cases} 1 - \dfrac{\epsilon}{|\mathcal{A}(s)|}(|\mathcal{A}(s)| - 1), & \text{对于最大价值动作} \\ \dfrac{\epsilon}{|\mathcal{A}(s)|}, & \text{对于其他}|\mathcal{A}(s)| - 1\text{个动作} \end{cases} $$$|\mathcal{A}(s)|$表示与$s$相关联的动作数量。
$\epsilon$-Greedy是随机性策略，它选择具有最大价值的动作的概率最高，而选择其他所有动作的概率都相同。 $$1 - \dfrac{\epsilon}{|\mathcal{A}(s)|}(|\mathcal{A}(s)| - 1) = 1 - \epsilon + \dfrac{\epsilon}{|\mathcal{A}(s)|} \geqslant \dfrac{\epsilon}{|\mathcal{A}(s)|}$$ 对于任意$\epsilon \in [0,1]$都成立。 
- 当$\epsilon = 0$时，$\epsilon$-Greedy变为普通的贪婪策略，此时策略的探索性最弱，因为只会选择最大价值动作。
- 当$\epsilon = 1$时，所有动作被选择的概率都等于$1/|\mathcal{A}(s)|$，此时策略的探索性最强。

MC Exploring Starts中策略改进的步骤是选择如下的Greedy策略： $$\pi_{k+1}(s) = \arg\max_{\pi \in \Pi} \sum_a \pi(a|s) q_k(s,a), \tag{5.4}$$ 其中$\Pi$表示所有可能策略的集合。我们知道(5.4)的最优解是一个贪婪策略： $$ \pi_{k+1}(a|s) = \begin{cases} 1, & a = a_k^* \\ 0, & a \neq a_k^* \end{cases} $$ 其中$a_k^* = \arg\max_a q_k(s,a)$。
现在，策略改进步骤需要改成 $$\pi_{k+1}(s) = \arg\max_{\pi \in \Pi_\epsilon} \sum_a \pi(a|s) q_k(s,a), \tag{5.5}$$ 其中$\Pi_\epsilon$表示所有$\epsilon$-Greedy策略的集合，这里$\epsilon$值是给定的。通过这种方式，我们强制把策略限制为$\epsilon$-Greedy。方程(5.5)的解不难得到： $$ \pi_{k+1}(a|s) = \begin{cases} 1 - \dfrac{|\mathcal{A}(s)| - 1}{|\mathcal{A}(s)|}\epsilon, & a = a_k^* \\ \dfrac{1}{|\mathcal{A}(s)|}\epsilon, & a \neq a_k^* \end{cases} $$其中$a_k^* = \arg\max_a q_{\pi_k}(s,a)$。
###### 伪代码
***
初始化：初始策略$\pi_0(a|s)$，所有$(s,a)$的初始值$q(s,a)$。$\text{Return}(s,a) = 0$和$\text{Number}(s,a) = 0$对于所有$(s,a)$。$\epsilon \in (0,1]$ 
目标：寻找最优策略。 
- 对每个回合 
	- 回合生成：选择一个初始状态-动作$(s_0, a_0)$（不需要每一个状态-动作都被选到）。执行当前策略，生成长度为$T$的回合：$s_0, a_0, r_0, r_1, \dots, s_{T-1}, a_{T-1}, r_T$。 每个回合的初始化：$g \leftarrow 0$ 
	- 对回合的每一步$t = T-1, T-2, \dots, 0$ 
		- $g \leftarrow \gamma g + r_{t+1}$ 
		- $\text{Return}(s_t, a_t) \leftarrow \text{Return}(s_t, a_t) + g$ 
		- $\text{Number}(s_t, a_t) \leftarrow \text{Number}(s_t, a_t) + 1$ 
		- 策略评估：
			- $q(s_t, a_t) \leftarrow \text{Return}(s_t, a_t)/\text{Number}(s_t, a_t)$ 
			- 策略改进：
				- 如果$a^* = \arg\max_a q(s_t, a)$，那么 $$ \pi(a|s_t) = \begin{cases} 1 - \dfrac{|\mathcal{A}(s_t)| - 1}{|\mathcal{A}(s_t)|}\epsilon, & a = a^* \\ \dfrac{1}{|\mathcal{A}(s_t)|}\epsilon, & a \neq a^* \end{cases} $$
#### 探索和利用
**探索**意味着策略会尝试尽可能多的动作，从而使得所有动作都能被良好地评估；“探索”是为了更好地评估，防止错过高价值的动作；

**利用**意味着策略会尽可能选取价值高的动作，从而充分利用当前价值评估的结果。“利用”是为了更好地利用评估的结果，否则难以得到最优的策略；

**权衡**的核心在于：当前的价值评估可能是不好的，我们要通过探索更好地评估价值。如果策略具有过多的探索性，则没有很好地利用评估结果，因此也离最优策略比较远；如果策略过多地利用当前的评估结果，则具有较少的探索性，难以全面评估所有动作。
   
当$\epsilon$减少时，$\epsilon$-Greedy策略的探索能力减弱。使用$\epsilon$-Greedy策略一个常见的技巧是，初期选取较大的$\epsilon$从而获得较强的探索性，末期选取较小的$\epsilon$从而获得较好的最优性。