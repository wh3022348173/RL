---
date: 2025-10-30
tags:
---
#### 罗宾斯-门罗算法(**RM算法**)
$g(w) = 0$,$w$是未知变量，输出值包含噪声情况$\tilde{g}(w,\eta) = g(w) + \eta$，$\eta$是观测噪声，可能不是高斯分布。（函数类似黑盒系统）
![[6-1.png|566]]
求解$g(w)=0$的RM算法是 $$w_{k+1} = w_k - a_k \tilde{g}(w_k, \eta_k),\quad k = 1,2,3,\ldots \tag{6.5}$$ 其中$w_k$是第$k$次方程解的估计值，$\tilde{g}(w_k, \eta_k)$是输入$w_k$后输出的观测值，$a_k$是一个正系数。
RM算法不需要关于函数表达式的任何信息，而仅需要输入和输出。 
##### 收敛性质
**定理6.1 (罗宾斯-门罗定理)**。在(6.5)的RM算法中，如果下面三个条件全部成立：
- (a) 存在两个常数$c_1, c_2 > 0$，使得$0 < c_1 \leqslant \nabla_w g(w) \leqslant c_2$对于所有的$w$成立；
- (b) $\sum_{k=1}^{\infty} a_k = \infty$且$\sum_{k=1}^{\infty} a_k^2 < \infty$；
- (c) $\mathbb{E}[\eta_k|\mathcal{H}_k] = 0$且$\mathbb{E}[\eta_k^2|\mathcal{H}_k] < \infty$，其中$\mathcal{H}_k = \{w_k, w_{k-1}, \ldots\}$； 那么$w_k$几乎必然收敛到$g(w^*) = 0$的根$w^*$。

**条件 (a)** 
- $0 < c_1 \leqslant \nabla_w g(w)$ 要求 $g(w)$ 是一个单调递增的函数。这个条件确保 $g(w) = 0$ 的根存在并且是唯一的。 如果 $g(w) \doteq \nabla_w J(w)$，那么 $c_1 \leqslant \nabla_w g(w)$ 等价于 $c_1 \leqslant \nabla_w^2 J(w)$，这意味着 $J(w)$ 是凸的（convex）如果 $g(w)$ 是单调递减的，我们可以简单地将 $-g(w)$ 视为一个新的单调递增的函数。 
- 不等式 $\nabla_w g(w) \leqslant c_2$ 表明 $g(w)$ 的梯度不能趋于无穷。
**条件 (b)** 是关于 $a_k$ 的，我们经常在强化学习算法中看到类似的条件。
- $\sum_{k=1}^{\infty} a_k^2 < \infty$ 意味着 $\lim_{n \to \infty} \sum_{k=1}^n a_k^2$ 是有上界的，这要求 $a_k$ 随着 $k \to \infty$ 收敛到0。
- $\sum_{k=1}^{\infty} a_k = \infty$ 意味着 $\lim_{n \to \infty} \sum_{k=1}^n a_k$ 是无限大的，这要求 $a_k$ 不能太快地收敛到0。
**条件 (c)** 是关于噪声 $\eta_k$ 的，它并不要求观测误差 $\eta_k$ 是高斯分布的。
- 该条件成立的一个重要特例是 $\{\eta_k\}$ 是一个**独立同分布**的随机序列，因此满足 $\mathbb{E}[\eta_k] = 0$ 且 $\mathbb{E}[\eta_k^2] < \infty$。此时，因为 $\eta_k$ 与 $\mathcal{H}_k$ 是独立的，所以 $\mathbb{E}[\eta_k|\mathcal{H}_k] = \mathbb{E}[\eta_k] = 0$ 且 $\mathbb{E}[\eta_k^2|\mathcal{H}_k] = \mathbb{E}[\eta_k^2]$。

#### Dvoretzky定理
**(Dvoretzky定理)**。考虑如下的随机过程： $$\Delta_{k+1} = (1 - \alpha_k)\Delta_k + \beta_k \eta_k,$$ 其中$\{\alpha_k\}_{k=1}^{\infty}$，$\{\beta_k\}_{k=1}^{\infty}$，$\{\eta_k\}_{k=1}^{\infty}$是随机序列，且$\alpha_k \geqslant 0$，$\beta_k \geqslant 0$。如果以下条件成立，那么$\Delta_k$将几乎必然收敛到0：
- (a) $\sum_{k=1}^{\infty} \alpha_k = \infty$，$\sum_{k=1}^{\infty} \alpha_k^2 < \infty$，$\sum_{k=1}^{\infty} \beta_k^2 < \infty$一致几乎必然成立；
- (b) $\mathbb{E}[\eta_k|\mathcal{H}_k] = 0$并且$\mathbb{E}[\eta_k^2|\mathcal{H}_k] \leqslant C$几乎必然成立。 其中$\mathcal{H}_k = \{\Delta_k, \Delta_{k-1}, \ldots, \eta_{k-1}, \ldots, \alpha_{k-1}, \ldots, \beta_{k-1}, \ldots\}$。
##### Dvoretzky定理推广
**定理6.3**。给定一个有限实数集合$S$。对$s \in S$有如下随机过程： $$\Delta_{k+1}(s) = (1 - \alpha_k(s))\Delta_k(s) + \beta_k(s)\eta_k(s).$$ 如果对于所有$s \in S$下面的条件都成立，那么$\Delta_k(s)$几乎必然收敛到0： 
- (a) $\sum_k \alpha_k(s) = \infty$，$\sum_k \alpha_k^2(s) < \infty$，$\sum_k \beta_k^2(s) < \infty$，并且$\mathbb{E}[\beta_k(s)|\mathcal{H}_k] \leqslant \mathbb{E}[\alpha_k(s)|\mathcal{H}_k]$一致几乎必然成立； 
- (b) $\|\mathbb{E}[\eta_k(s)|\mathcal{H}_k]\|_\infty \leqslant \gamma\|\Delta_k\|_\infty$，其中$\gamma \in (0,1)$； 
- (c) $\text{var}[\eta_k(s)|\mathcal{H}_k] \leqslant C(1 + \|\Delta_k(s)\|_\infty)^2$，这里$C$是一个常数。 其中$\mathcal{H}_k = \{\Delta_k, \Delta_{k-1}, \ldots, \eta_{k-1}, \ldots, \alpha_{k-1}, \ldots, \beta_{k-1}, \ldots\}$代表历史信息，$\|\cdot\|_\infty$代表最大范数。
#### 随机梯度下降（SGD）- 特殊RM
考虑如下优化问题： $$\min_w J(w) = \mathbb{E}[f(w, X)], \tag{6.10}$$ 其中$w$是需要优化的参数，$X$是一个随机变量，这里期望值是针对$X$的。这里$w$和$X$可以是标量或者向量，而函数$f(\cdot)$是一个标量。 
梯度下降算法是 $$w_{k+1} = w_k - \alpha_k \nabla_w J(w_k) = w_k - \alpha_k \mathbb{E}[\nabla_w f(w_k, X)]. \tag{6.11}$$ 梯度下降算法在一些条件下可以找到全局最优解$w^*$。
1. 基于模型的方法，即利用$X$的概率分布和期望值的定义直接计算，然而$X$的概率分布可能是难以得到的；
2. 基于数据的方法，即利用独立同分布的样本$\{x_i\}_{i=1}^n$来计算： $$\mathbb{E}[\nabla_w f(w_k, X)] \approx \frac{1}{n}\sum_{i=1}^n \nabla_w f(w_k, x_i).$$ 此时(6.11)变为 $$w_{k+1} = w_k - \frac{\alpha_k}{n}\sum_{i=1}^n \nabla_w f(w_k, x_i). \tag{6.12}$$ 算法(6.12)的问题在于它在每次迭代中都需要所有样本。如果样本是**逐个得到**的，则可以用下面的算法： $$w_{k+1} = w_k - \alpha_k \nabla_w f(w_k, x_k). \tag{6.13}$$ 其中$x_k$是在时刻$k$得到的样本。上式就是**随机梯度下降算法**，它用一个基于单个随机样本得到的**随机梯度**$\nabla_w f(w_k, x_k)$来替换梯度下降算法中的**真实梯度**$\mathbb{E}[\nabla_w f(w_k, X)]$。替换后该算法能确保当$k \to \infty$时$w_k \to w^*$。
##### 收敛模式
当估计值$w_k$远离最优解$w^*$时，随机梯度下降的收敛与常规梯度下降算法类似；只有当$w_k$接近$w^*$时，随机梯度下降的收敛才会表现出更多的随机性。
$$\delta_k \leqslant \frac{\overbrace{|\nabla_w f(w_k, x_k) - \mathbb{E}[\nabla_w f(w_k, X)]|}^{\text{随机梯度与真实梯度的差}}}{\underbrace{c|w_k - w^*|}_{\text{估计的绝对误差}}}.$$ 上式表明了梯度的相对误差$\delta_k$与估计的绝对误差$|w_k - w^*|$呈反比。
- 当$|w_k - w^*|$较大时，$\delta_k$较小，此时随机梯度和真实梯度差不多，因此随机梯度下降算法的表现类似于梯度下降算法，进而$w_k$会迅速接近$w^*$，这是我们希望看到的良好性质。
- 当$|w_k - w^*|$较小时，梯度的相对误差$\delta_k$可能会很大，此时随机梯度下降算法的表现相比梯度下降会有更大的随机性，这是使用随机样本难以避免的问题，可以通过使用更多的数据和较小的学习率来缓解随机性问题。
##### 另一种描述
考虑一组实数$\{x_i\}_{i=1}^n$，其中$x_i$并非任何随机变量的样本。这里要解决的优化问题是最小化平均值： $$\min_w J(w) = \frac{1}{n}\sum_{i=1}^n f(w, x_i), \tag{6.16}$$ 其中$f(w, x_i)$是一个参数化的函数。解决这个问题的梯度下降算法是 $$w_{k+1} = w_k - \alpha_k \nabla_w J(w_k) = w_k - \alpha_k \frac{1}{n}\sum_{i=1}^n \nabla_w f(w_k, x_i).$$ 如果集合$\{x_i\}_{i=1}^n$很大，并且我们每次只能获取一个元素，此时可以用下式来更新$w_k$： $$w_{k+1} = w_k - \alpha_k \nabla_w f(w_k, x_k). \tag{6.17}$$ 注意这里的$x_k$是在时刻$k$得到的值，并非集合$\{x_i\}_{i=1}^n$中的第$k$个元素。 
问题不涉及任何随机变量或期望值。人为引入随机变量，将上述表述转换为一个我们熟悉的涉及随机变量的描述。具体来说，设$X$为定义在集合$\{x_i\}_{i=1}^n$上的随机变量。假设其概率分布是均匀的，即$p(X = x_i) = 1/n$。优化问题就变成了 $$\min_w J(w) = \frac{1}{n}\sum_{i=1}^n f(w, x_i) = \mathbb{E}[f(w, X)].$$ 上式中的第二个等号是严格成立的而不是近似。
##### 小批量梯度下降
**小批量梯度下降（mini-batch gradient descent，MBGD）**，在每次迭代中会使用更多的样本。
如果每次迭代使用所有样本，那么该算法被称为**批量梯度下降（batch gradient descent，BGD）**。
$$w_{k+1} = w_k - \alpha_k \frac{1}{n}\sum_{i=1}^n \nabla_w f(w_k, x_i), \quad \text{(BGD)}$$ $$w_{k+1} = w_k - \alpha_k \frac{1}{m}\sum_{j \in \mathcal{I}_k} \nabla_w f(w_k, x_j), \quad \text{(MBGD)}$$ $$w_{k+1} = w_k - \alpha_k \nabla_w f(w_k, x_k). \quad \text{(SGD)}$$
1. BGD算法在每一次迭代中都用到了所有样本，因此$(1/n)\sum_{i=1}^n \nabla_w f(w_k, x_i)$也更加接近于真实梯度$\mathbb{E}[\nabla_w f(w_k, X)]$。
2. MBGD算法在每次迭代中仅使用一小批样本，这批样本的集合记作$\mathcal{I}_k$，这批样本也是独立同分布的，样本数量记作$|\mathcal{I}_k| = m$。
3. SGD算法在每次迭代中仅用到一个样本$x_k$，它是在时刻$k$随机从$\{x_i\}_{i=1}^n$中采样得到的。
4. MBGD可以被视为SGD和BGD之间的中间版本。与SGD相比，MBGD的随机性较小，收敛速度通常更快，因为它使用的样本比SGD更多。与BGD相比，MBGD不需要在每次迭代中使用所有样本，因此更加灵活。
##### 随机梯度下降的收敛性
**定理6.4 (随机梯度下降的收敛性)**。对于(6.13)中的随机梯度下降算法，如果下列条件都成立，那么$w_k$几乎必然收敛到$\nabla_w \mathbb{E}[f(w, X)] = 0$的解：
- (a) $0 < c_1 \leqslant \nabla_w^2 f(w, X) \leqslant c_2$对任意$X$都成立；
- (b) $\sum_{k=1}^{\infty} a_k = \infty$，$\sum_{k=1}^{\infty} a_k^2 < \infty$； 
- (c) $\{x_k\}_{k=1}^{\infty}$是独立同分布的。

**条件 (a)** 要求$f$是凸的，并且其二阶导数是有上下界的。这里$w$和$\nabla_w^2 f(w, X)$都是标量。向量情况：当$w$是一个向量时，$\nabla_w^2 f(w, X)$是海森（Hessian）矩阵。 
**条件 (b)** 与RM算法一致。实践中$a_k$常被选择为一个很小的正数，此时$\sum_{k=1}^{\infty} a_k^2 < \infty$不再成立。
**条件 (c)** 是一个常见的假设条件。